{
  "input_docs": 141,
  "output_chunks": 9752,
  "dropped_small_chunks": 0,
  "docs_missing_text_or_id": 4,
  "tokenizer_model": "sentence-transformers/all-MiniLM-L6-v2",
  "tokenizer_available": true,
  "params": {
    "target_tokens": 320,
    "overlap_tokens": 80,
    "hard_max_tokens": 480,
    "min_tokens": 80
  },
  "out_path": "corpus/derived/text/chunks.jsonl"
}