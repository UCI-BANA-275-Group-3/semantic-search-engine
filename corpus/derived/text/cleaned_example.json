{
  "doc_id": "MJ58ZT8Z:4461",
  "item_key": "MJ58ZT8Z",
  "attachment_key": "4461",
  "title": "Prediction, Judgment, and Complexity: A Theory of Decision-Making and Artificial Intelligence",
  "creators": [
    "Agrawal, Ajay",
    "Gans, Joshua",
    "Goldfarb, Avi"
  ],
  "year": 2018,
  "collection_keys": [
    "MCBRD7KR"
  ],
  "collection_names": [
    "S02 - AI and Individual Decision Making"
  ],
  "source_metadata": "library.json",
  "source_item_index": 1,
  "attachment_path": "corpus/raw/zotero/storage/4461/Agrawal et al. - 2018 - Prediction, Judgment, and Complexity A Theory of Decision-Making and Artificial Intelligence.pdf",
  "attachment_filename": "Agrawal et al. - 2018 - Prediction, Judgment, and Complexity A Theory of Decision-Making and Artificial Intelligence.pdf",
  "attachment_ext": ".pdf",
  "mime": "application/pdf",
  "size_bytes": 906868,
  "abstract": null,
  "tags": [],
  "doi": null,
  "url": "https://www.nber.org/books-and-chapters/economics-artificial-intelligence-agenda/prediction-judgment-and-complexity-theory-decision-making-and-artificial-intelligence",
  "date_added": "2025-09-29T22:34:36Z",
  "date_modified": "2025-09-29T22:34:36Z",
  "language": null,
  "raw_text": "This PDF is a selection from a published volume from the National Bureau\nof Economic Research\n\n\nVolume Title: The Economics of Artificial Intelligence: An Agenda\n\n\nVolume Authors/Editors: Ajay Agrawal, Joshua Gans, and Avi Goldfarb,\neditors\n\n\nVolume Publisher: University of Chicago Press\n\n\nVolume ISBNs: 978-0-226-61333-8 (cloth); 978-0-226-61347-5 (electronic)\n\n\nVolume URL: http://www.nber.org/books/agra-1\n\n\nConference Date: September 13–14, 2017\n\n\nPublication Date: May 2019\n\n\nChapter Title: Prediction, Judgment, and Complexity: A Theory of\nDecision-Making and Artificial Intelligence\n\n\nChapter Author(s): Ajay Agrawal, Joshua Gans, Avi Goldfarb\n\n\nChapter URL: http://www.nber.org/chapters/c14010\n\n\nChapter pages in book: (p. 89 – 110)\n\n\n# **and Complexity** A Theory of Decision- Making and Artifi cial Intelligence\n\nAjay Agrawal, Joshua Gans, and Avi Goldfarb\n\n\n**3.1 Introduction**\n\n\nThere is widespread discussion regarding the impact of machines on\nemployment (see Autor 2015). In some sense, the discussion mirrors a longstanding literature on the impact of the accumulation of capital equipment\non employment; specifi cally, whether capital and labor are substitutes or\ncomplements (Acemoglu 2003). But the recent discussion is motivated by\nthe integration of software with hardware and whether the role of machines\ngoes beyond physical tasks to mental ones as well (Brynjolfsson and McAfee\n2014). As mental tasks were seen as always being present and essential,\nhuman comparative advantage in these was seen as the main reason why, at\nleast in the long term, capital accumulation would complement employment\nby enhancing labor productivity in those tasks.\nThe computer revolution has blurred the line between physical and men\n\nAjay Agrawal is the Peter Munk Professor of Entrepreneurship at the Rotman School of\nManagement, University of Toronto, and a research associate of the National Bureau of Economic Research. Joshua Gans is professor of strategic management and holder of the Jeff rey S.\nSkoll Chair of Technical Innovation and Entrepreneurship at the Rotman School of Management, University of Toronto (with a cross appointment in the Department of Economics),\nand a research associate of the National Bureau of Economic Research. Avi Goldfarb holds\nthe Rotman Chair in Artifi cial Intelligence and Healthcare and is professor of marketing at\nthe Rotman School of Management, University of Toronto, and is a research associate of the\nNational Bureau of Economic Research.\nOur thanks to Andrea Prat, Scott Stern, Hal Varian, and participants at the AEA (Chicago),\nNBER Summer Institute (2017), NBER Economics of AI Conference (Toronto), Columbia\nLaw School, Harvard Business School, MIT, and University of Toronto for helpful comments.\nResponsibility for all errors remains our own. The latest version of this chapter is available\nat joshuagans .com. For acknowledgments, sources of research support, and disclosure of\nthe authors’ material fi nancial relationships, if any, please see http:// www .nber .org/ chapters\n/ c14010.ack.\n\n\nYou are reading copyrighted material published by University of Chicago Press. **89**\nUnauthorized posting, copying, or distributing of this work except as permitted under\nU.S. copyright law is illegal and injures the author and publisher.\n\n\n**90  Ajay Agrawal, Joshua Gans, and Avi Goldfarb**\n\n\ntal tasks. For instance, the invention of the spreadsheet in the late 1970s\nfundamentally changed the role of bookkeepers. Prior to that invention,\nthere was a time- intensive task involving the recomputation of outcomes in\nspreadsheets as data or assumptions changed. That human task was substituted by the spreadsheet software that could produce the calculations more\nquickly, cheaply, and frequently. However, at the same time, the spreadsheet\nmade the jobs of accountants, analysts, and others far more productive.\nIn the accounting books, capital was substituting for labor, but the mental\nproductivity of labor was being changed. Thus, the impact on employment\ncritically depended on whether there were tasks the “computers cannot do.”\nThese assumptions persist in models today. Acemoglu and Restrepo\n(2017) observe that capital substitutes for labor in certain tasks while at the\nsame time technological progress creates new tasks. They make what they\ncall a “natural assumption” that only labor can perform the new tasks as\nthey are more complex than previous ones. [1] Benzell et al. (2015) consider\nthe impact of software more explicitly. Their environment has two types of\nlabor—high- tech (who can, among other things, code) and low- tech (who\nare empathetic and can handle interpersonal tasks). In this environment,\nit is the low- tech workers who cannot be replaced by machines while the\nhigh- tech ones are employed initially to create the code that will eventually\ndisplace their kind. The results of the model depend, therefore, on a class\nof worker who cannot be substituted directly for capital, but also on the\ninability of workers themselves to substitute between classes.\nIn this chapter, our approach is to delve into the weeds of what is happening currently in the fi eld of artifi cial intelligence (AI). The recent wave\nof developments in AI all involve advances in machine learning. Those\nadvances allow for automated and cheap prediction; that is, providing a\nforecast (or nowcast) of a variable of interest from available data (Agrawal,\nGans and Goldfarb 2018b). In some cases, prediction has enabled full automation of tasks—for example, self- driving vehicles where the process of\ndata collection, prediction of behavior and surroundings, and actions are\nall conducted without a human in the loop. In other cases, prediction is a\nstandalone tool—such as image recognition or fraud detection—that may\nor may not lead to further substitution of human users of such tools by\nmachines. Thus far, substitution between humans and machines has focused\nmainly on cost considerations. Are machines cheaper, more reliable, and\nmore scalable (in their software form) than humans? This chapter, however,\nconsiders the role of prediction in decision- making explicitly and from that\nexamines the complementary skills that may be matched with prediction\nwithin a task.\n\n\n1. To be sure, their model is designed to examine how automation of tasks causes a change\nin factor prices that biases innovation toward the creation of new tasks that labor is more\nsuited to.\n\n\nYou are reading copyrighted material published by University of Chicago Press.\nUnauthorized posting, copying, or distributing of this work except as permitted under\nU.S. copyright law is illegal and injures the author and publisher.\n\n\nA Theory of Decision- Making and Artifi cial Intelligence **91**\n\n\nOur focus, in this regard, is on what we term _judgment_ . While judgment\nis a term with broad meaning, here we use it to refer to a very specifi c skill.\nTo see this, consider a decision. That decision involves choosing an action,\n_x_, from a set, _X_ . The payoff (or reward) from that action is defi ned by a\nfunction, _u_ ( _x_, �) where � is a realization of an uncertain state drawn from\na distribution, _F_ (�). Suppose that, prior to making a decision, a _prediction_\n(or signal), _s_, can be generated that results in a posterior, _F_ (�| _s_ ). Thus, the\ndecision maker would solve\n\n\nIn other words, a standard problem of choice under uncertainty. In this\nstandard world, the role of prediction is to improve decision- making. The\npayoff, or utility function, is known.\nTo create a role for judgment, we depart from this standard set-up in\nstatistical decision theory and ask how a decision maker comes to know the\nfunction, _u_ ( _x_, �)? We assume that this is not simply given or a primitive of the\ndecision- making model. Instead, it requires a human to undertake a costly\nprocess that allows the mapping from ( _x_, �) to a particular payoff value, _u_, to\nbe discovered. This is a reasonable assumption given that beyond some rudimentary experimentation in closed environments, there is no current way for\nan AI to impute a utility function that resides with humans. Additionally,\nthis process separates the costs of providing the mapping for each pair, ( _x_, �).\n(Actually, we focus, without loss in generality, on situations where _u_ ( _x_, �) ≠\n_u_ ( _x_ ) for all � and presume that if a payoff to an action is state independent\nthat payoff is known.) In other words, while prediction can obtain a signal\nof the underlying state, judgment is the process by which the payoff s from\nactions that arise based on that state can be determined. We assume that\nthis process of determining payoff s requires human understanding of the\nsituation: it is not a prediction problem.\nFor intuition on the diff erence between prediction and judgment, consider\nthe example of credit card fraud. A bank observes a credit card transaction.\nThat transaction is either legitimate or fraudulent. The decision is whether\nto approve the transaction. If the bank knows for sure that the transaction\nis legitimate, the bank will approve it. If the bank knows for sure that it is\nfraudulent, the bank will refuse the transaction. Why? Because the bank\nknows the payoff of approving a legitimate transaction is higher than the\npayoff of refusing that transaction. Things get more interesting if the bank\nis uncertain about whether the transaction is legitimate. The uncertainty\nmeans that the bank also needs to know the payoff from refusing a legitimate\ntransaction and from approving a fraudulent transaction. In our model,\njudgment is the process of determining these payoff s. It is a costly activity,\nin the sense that it requires time and eff ort.\nAs the new developments regarding AI all involve making prediction\nmore readily available, we ask, how does judgment and its endogenous appli\n\nYou are reading copyrighted material published by University of Chicago Press.\nUnauthorized posting, copying, or distributing of this work except as permitted under\nU.S. copyright law is illegal and injures the author and publisher.\n\n\n**92  Ajay Agrawal, Joshua Gans, and Avi Goldfarb**\n\n\ncation change the value of prediction? Are prediction and judgment substitutes or complements? How does the value of prediction change monotonically with the diffi culty of applying judgment? In complex environments\n(as they relate to automation, contracting, and the boundaries of the fi rm),\nhow do improvements in prediction aff ect the value of judgment?\nWe proceed by fi rst providing supportive evidence for our assumption that\nrecent developments in AI overwhelmingly impact the costs of prediction.\nWe then use the example of radiology to provide a context for understanding the diff erent roles of prediction and judgment. Drawing inspiration from\nBolton and Faure- Grimaud (2009), we then build the baseline model with\ntwo states of the world and uncertainty about payoff s to actions in each\nstate. We explore the value of judgment in the absence of any prediction\ntechnology, and then the value of prediction technology when there is no\njudgment. We fi nish the discussion of the baseline model with an exploration of the interaction between prediction and judgment, demonstrating\nthat prediction and judgment are complements as long as judgment isn’t too\ndiffi cult. We then separate prediction quality into prediction frequency and\nprediction accuracy. As judgment improves, accuracy becomes more important relative to frequency. Finally, we examine complex environments where\nthe number of potential states is large. Such environments are common in\neconomic models of automation, contracting, and boundaries of the fi rm.\nWe show that the eff ect of improvements in prediction on the importance\nof judgment depend a great deal on whether the improvements in prediction\nenable automated decision- making.\n\n\n**3.2 AI and Prediction Costs**\n\n\nWe argue that the recent advances in artifi cial intelligence are advances\nin the technology of prediction. Most broadly, we defi ne prediction as the\nability to take known information to generate new information. Our model\nemphasizes prediction about the state of the world.\nMost contemporary artifi cial intelligence research and applications come\nfrom a fi eld now called “machine learning.” Many of the tools of machine\nlearning have a long history in statistics and data analysis, and are likely\nfamiliar to economists and applied statisticians as tools for prediction and\nclassifi cation. [2] For example, Alpaydin’s (2010) textbook _Introduction to_\n_Machine Learning_ covers maximum likelihood estimation, Bayesian estimation, multivariate linear regression, principal components analysis, clustering, and nonparametric regression. In addition, it covers tools that may\nbe less familiar, but also use independent variables to predict outcomes:\n\n\n2. We defi ne prediction as known information to generate new information. Therefore, classifi cation techniques such as clustering are prediction techniques in which the new information\nto be predicted is the appropriate category or class.\n\n\nYou are reading copyrighted material published by University of Chicago Press.\nUnauthorized posting, copying, or distributing of this work except as permitted under\nU.S. copyright law is illegal and injures the author and publisher.\n\n\nA Theory of Decision- Making and Artifi cial Intelligence **93**\n\n\nregression trees, neural networks, hidden Markov models, and reinforcement learning. Hastie, Tibshirani, and Friedman (2009) cover similar topics.\nThe 2014 _Journal of Economic Perspectives_ symposium on big data covered\nseveral of these less familiar prediction techniques in articles by Varian\n(2014) and Belloni, Chernozhukov, and Hansen (2014).\nWhile many of these prediction techniques are not new, recent advances\nin computer speed, data collection, data storage, and the prediction methods\nthemselves have led to substantial improvements. These improvements have\ntransformed the computer science research fi eld of artifi cial intelligence. The\nOxford English Dictionary defi nes artifi cial intelligence as “[t]he theory and\ndevelopment of computer systems able to perform tasks normally requiring\nhuman intelligence.” In the 1960s and 1970s, artifi cial intelligence research\nwas primarily rules- based, symbolic logic. It involved human experts generating rules that an algorithm could follow (Domingos 2015, 89). These\nare not prediction technologies. Such systems became very good chess\nplayers and they guided factory robots in highly controlled settings; however, by the 1980s, it became clear that rules- based systems could not deal\nwith the complexity of many nonartifi cial settings. This led to an “AI winter”\nin which research funding artifi cial intelligence projects largely dried up\n(Markov 2015).\nOver the past ten years, a diff erent approach to artifi cial intelligence has\ntaken off . The idea is to program computers to “learn” from example data\nor experience. In the absence of the ability to predetermine the decision\nrules, a data- driven prediction approach can conduct many mental tasks.\nFor example, humans are good at recognizing familiar faces, but we would\nstruggle to explain and codify this skill. By connecting data on names to\nimage data on faces, machine learning solves this problem by predicting\nwhich image data patterns are associated with which names. As a prominent\nartifi cial intelligence researcher put it, “Almost all of AI’s recent progress is\nthrough one type, in which some input data (A) is used to quickly generate\nsome simple response (B)” (Ng 2016). Thus, the progress is explicitly about\nimprovements in prediction. In other words, the suite of technologies that\nhave given rise to the recent resurgence of interest in artifi cial intelligence\nuse data collected from sensors, images, videos, typed notes, or anything\nelse that can be represented in bits to fi ll in missing information, recognize\nobjects, or forecast what will happen next.\nTo be clear, we do not take a position on whether these prediction technologies really do mimic the core aspects of human intelligence. While Palm\nComputing founder Jeff Hawkins argues that human intelligence is—in\nessence—prediction (Hawkins 2004), many neuroscientists, psychologists,\nand others disagree. Our point is that the technologies that have been given\nthe label artifi cial intelligence are prediction technologies. Therefore, in\norder to understand the impact of these technologies, it is important to\nassess the impact of prediction on decisions.\n\n\nYou are reading copyrighted material published by University of Chicago Press.\nUnauthorized posting, copying, or distributing of this work except as permitted under\nU.S. copyright law is illegal and injures the author and publisher.\n\n\n**94  Ajay Agrawal, Joshua Gans, and Avi Goldfarb**\n\n\n**3.3 Case: Radiology**\n\n\nBefore proceeding to the model, we provide some intuition of how prediction and judgment apply in a particular context where prediction machines\nare expected to have a large impact: radiology. In 2016, Geoff Hinton—one\nof the pioneers of deep learning neural networks—stated that it was no longer worth training radiologists. His strong implication was that radiologists\nwould not have a future. This is something that radiologists have been concerned about since 1960 (Lusted 1960). Today, machine- learning techniques\nare being heavily applied in radiology by IBM using its Watson computer\nand by a start-up, Enlitic. Enlitic has been able to use deep learning to detect\nlung nodules (a fairly routine exercise) [3] but also fractures (which is more\ncomplex). Watson can now identify pulmonary embolism and some other\nheart issues. These advances are at the heart of Hinton’s forecast, but have\nalso been widely discussed among radiologists and pathologists (Jha and\nTopol 2016). What does the model in this chapter suggest about the future\nof radiologists?\nIf we consider a simplifi ed characterization of the job of a radiologist,\nit would be that they examine an image in order to characterize and classify that image and return an assessment to a physician. While often that\nassessment is a diagnosis (i.e., “the patient has pneumonia”), in many cases,\nthe assessment is in the negative (i.e., “pneumonia not excluded”). In that\nregard, this is stated as a predictive task to inform the physician of the\nlikelihood of the state of the world. Using that, the physician can devise a\ntreatment.\nThese predictions are what machines are aiming to provide. In particular,\nit might provide a diff erential diagnosis of the following kind:\n\n\n_Based on Mr Patel’s demographics and imaging, the mass in the liver has a_\n_66.6 percent chance of being benign, 33.3 percent chance of being malignant,_\n_and a 0.1 percent of not being real._ [4]\n\n\nThe action is whether some intervention is needed. For instance, if a\npotential tumor is identifi ed in a noninvasive scan, then this will inform\nwhether an invasive examination will be conducted. In terms of identifying\nthe state of the world, the invasive exam is costly but safe—it can deduce a\ncancer with certainty and remove it if necessary. The role of a noninvasive\nexam is to inform whether an invasive exam should be forgone. That is, it\nis to make physicians more confi dent about abstaining from treatment and\nfurther analysis. In this regard, if the machine improves prediction, it will\nlead to fewer invasive examinations.\n\n\n3. “You did not go to medical school to measure lung nodules.” http:// www .medscape .com\n/ viewarticle/ 863127#vp_2.\n4. http:// www .medscape .com/ viewarticle/ 863127#vp_3.\n\n\nYou are reading copyrighted material published by University of Chicago Press.\nUnauthorized posting, copying, or distributing of this work except as permitted under\nU.S. copyright law is illegal and injures the author and publisher.\n\n\nA Theory of Decision- Making and Artifi cial Intelligence **95**\n\n\nJudgment involves understanding the payoff s. What is the payoff to conducting a biopsy if the mass is benign, malignant, or not real? What is the\npayoff to not doing anything in those three states? The issue for radiologists\nin particular is whether a trained specialist radiologist is in the best position\nto make this judgment or will it occur further along the chain of decisionmaking or involve new job classes that merge diagnostic information such\nas a combined radiologist/ pathologist (Jha and Topol 2016). Next, we formalize these ideas.\n\n\n**3.4 Baseline Model**\n\n\nOur baseline model is inspired by the “bandit” environment considered by\nBolton and Faure- Grimaud (2009), although it departs signifi cantly in the\nquestions addressed and base assumptions made. Like them, in our baseline model, we suppose there are two states of the world, {�1,�2} with prior\nprobabilities of {�,1 – �}. There are two possible actions: a state independent action with known payoff of _S_ (safe) and a state dependent action with\ntwo possible payoff s, _R_ or _r,_ as the case may be (risky).\nAs noted in the introduction, a key departure from the usual assumptions of rational decision- making is that the decision maker does not know\nthe payoff from the risky action in each state and must apply _judgment_ to\ndetermine that payoff . [5] Moreover, decision makers need to be able to make\na judgment for each state that might arise in order to formulate a plan that\nwould be the equivalent of payoff maximization. In the absence of such\njudgment, the ex ante expectation that the risky action is optimal in any state\nis _v_ (which is independent between states). To make things more concrete,\nwe assume _R_ - _S_ - _r_ . [6] Thus, we assume that _v_ is the probability in any state\nthat the risky payoff is _R_ rather than _r_ . This is not a conditional probability\nof the state. It is a statement about the payoff, given the state.\nIn the absence of knowledge regarding the specifi c payoff s from the risky\naction, a decision can only be made on the basis of prior probabilities. Then\nthe safe action will be chosen if\n\nμ _vR_ ( + (1 _v_ ) _r_ ) + 1( μ) _vR_ ( + (1 _v_ ) _r_ ) = _vR_ + (1 _v_ ) _r_ _S_ .\n\n\n5. Bolton and Faure- Grimaud (2009) consider this step to be the equivalent of a thought\nexperiment where thinking takes time. To the extent that our results can be interpreted as a\nstatement about the comparative advantage of humans, we assume that only humans can do\njudgment.\n6. Thus, we assume that the payoff function, _u_, can only take one of three values, { _R_, _r_, _S_ }.\nThe issue is which combinations of state realization and action lead to which payoff s. However,\nwe assume that _S_ is the payoff from the safe action regardless of state and so this is known to the\ndecision maker. As it is the relative payoff s from actions that drive the results, this assumption\nis without loss in generality. Requiring this property of the safe action to be discovered would\njust add an extra cost. Implicitly, as the decision maker cannot make a decision in complete\nignorance, we are assuming that the safe action’s payoff can be judged at an arbitrarily low cost.\n\n\nYou are reading copyrighted material published by University of Chicago Press.\nUnauthorized posting, copying, or distributing of this work except as permitted under\nU.S. copyright law is illegal and injures the author and publisher.\n\n\n**96  Ajay Agrawal, Joshua Gans, and Avi Goldfarb**\n\n\nSo that the payoff is: _V_ 0 = max{ _vR_ + (1 – _v_ ) _r_, _S_ }. To make things simpler, we\nwill focus our attention on the case where the safe action is—in the absence\nof prediction or judgment—the default. That is, we assume that\n\n\n(A1) **(Safe Default)** _vR_ + (1 – _v_ ) _r_ ≤ _S_ .\n\n\nThis assumption is made for simplicity only and will not change the qualitative conclusions. [7] Under (A1), in the absence of knowledge of the payoff\nfunction or a signal of the state, the decision maker would choose _S_ .\n\n\n3.4.1 Judgment in the Absence of Prediction\n\n\nPrediction provides knowledge of the state. The process of judgment provides knowledge of the payoff function. Judgment therefore allows the decision maker to understand which action is optimal for a given state should\nit arise. Suppose that this knowledge is gained without cost (as it would be\nassumed to do under the usual assumptions of economic rationality). In\nother words, the decision maker has knowledge of optimal action in a given\nstate. Then the risky action will be chosen (a) if it is the preferred action in\nboth states (which arises with probability _v_ [2] ); (b) if it is the preferred action\nin �1 but not �2 and � _R_ + (1 – �) _r_ - _S_ (with probability _v_ (1 – _v_ )); or (c) if it is\nthe preferred action in �2 but not �1 and � _r_ + (1 – �) _R_ - _S_ (with probability\n_v_ (1 – _v_ )). Thus, the expected payoff is\n## v [2] R + v (1 v )max μ{ R + (1 μ) r, S } + v (1 v )max μ{ r + (1 μ) R, S } [+][ (1] v ) [2] S .\n\n\nNote that this is greater than _V_ 0. The reason for this is that, when there is\nuncertainty, judgment is valuable because it can identify actions that are\ndominant or dominated—that is, that might be optimal across states. In\nthis situation, any resolution of uncertainty does not matter as it will not\nchange the decision made.\nA key insight is that judgment itself can be consequential.\nResult 1: _If_ max{� _R_ + (1 – �) _r,_  - _r_ + (1 – �) _R_ } > _S, it is possible that_\n_judgment alone can cause the decision to switch from the default action (safe)_\n_to the alternative action (risky)._\nAs we are motivated by understanding the interplay between prediction\nand judgment, we want to make these consequential. Therefore, we make the\nfollowing assumption to ensure prediction always has some value:\n\n\n(A2) **(Judgment Insuffi cient)** max{� _R_ + (1 – �) _r,_ - _r_ + (1 – �) _R_ } ≤ _S_ .\n\n\nUnder this assumption, if diff erent actions are optimal in each state and\nthis is known, the decision maker will not change to the risky action. This,\nof course, implies that the expected payoff is\n\n\n7. Bolton and Faure- Grimaud (2009) make the opposite assumption. Here, as our focus is on\nthe impact of prediction, it is better to consider environments where prediction has the eff ect\nof reducing uncertainty over riskier actions.\n\n\nYou are reading copyrighted material published by University of Chicago Press.\nUnauthorized posting, copying, or distributing of this work except as permitted under\nU.S. copyright law is illegal and injures the author and publisher.\n\n\nA Theory of Decision- Making and Artifi cial Intelligence **97**\n\n\n_v_ [2] _R_ + (1 _v_ [2] ) _S_ .\n\n\nNote that, absent any cost, full judgment improves the decision maker’s\nexpected payoff .\nJudgment does not come for free. We assume here that it takes time\n(although the formulation would naturally match with the notion that it\ntakes costly eff ort). Suppose the discount factor is � < 1. A decision maker\ncan spend time in a period determining what the optimal action is for a particular state. If they choose to apply judgment with respect to state � _i_, then\nthere is a probability � _i_ that they will determine the optimal action in that\nperiod and can make a choice based on that judgment. Otherwise, they can\nchoose to apply judgment to that problem in the next period.\nIt is useful, at this point, to consider what judgment means once it has\nbeen applied. The initial assumption we make here is that the knowledge\nof the payoff function depreciates as soon as a decision is made. In other\nwords, applying judgment can delay a decision (and that is costly) and it\ncan improve that decision (which is its value) but it cannot generate experience that can be applied to other decisions (including future ones). In other\nwords, the initial conception of judgment is the application of _thought_ rather\nthan the gathering of _experience_ . [8] Practically, this reduces our examination\nto a static model. However, in a later section, we consider the experience\nformulation and demonstrate that most of the insights of the static model\ncarry over to the dynamic model.\nIn summary, the timing of the game is as follows:\n\n\n1. At the beginning of a decision stage, the decision maker chooses\nwhether to apply judgment and to what state or whether to simply choose\nan action without judgment. If an action is chosen, uncertainty is resolved\nand payoff s are realized and we move to a new decision stage.\n2. If judgment is chosen, with probability, 1 – � _i_, they do not fi nd out\nthe payoff s for the risky action in that state, a period of time elapses and\nthe game moves back to 1. With probability � _i_, the decision maker gains\nthis knowledge. The decision maker can then take an action, uncertainty\nis resolved and payoff s are realized, and we move to a new decision stage\n(back to 1). If no action is taken, a period of time elapses and the current\ndecision stage continues.\n3. The decision maker chooses whether to apply judgment to the other\nstate. If an action is chosen, uncertainty is resolved and payoff s are realized\nand we move to a new decision stage (back to 1).\n4. If judgment is chosen, with probability, 1 – �– _i_, they do not fi nd out\nthe payoff s for the risky action in that state, a period of time elapses and\nthe game moves back to 1. With probability �– _i_, the decision maker gains\nthis knowledge. The decision maker then chooses an action, uncertainty\n\n\n8. The experience frame is considered in Agrawal, Gans, and Goldfarb (2018a).\n\n\nYou are reading copyrighted material published by University of Chicago Press.\nUnauthorized posting, copying, or distributing of this work except as permitted under\nU.S. copyright law is illegal and injures the author and publisher.\n\n\n**98  Ajay Agrawal, Joshua Gans, and Avi Goldfarb**\n\n\n**Table 3.1** **Model parameters**\n\n\nParameter Description\n\n\n_S_ Known payoff from the safe action\n_R_ Potential payoff from the risky action in a given state\n_r_ Potential payoff from the risky action in a given state\nθ _i_ Label of state _i_ ∈ {1,2}\n\n- Probability of state 1\n_v_ Prior probability that the payoff in a given state is _R_\nλ _i_ Probablilty that decision maker learns the payoff to the risky action θ _i_ if\njudgment is applied for one period\nδ Discount factor\n\n\nis resolved and payoff s are realized, and we move to a new decision stage\n(back to 1).\n\n\nWhen prediction is available, it will become available prior to the beginning of a decision stage. The various parameters are listed in table 3.1.\nSuppose that the decision maker focuses on judging the optimal action\n(i.e., assessing the payoff ) for � _i_ . Then the expected present discount payoff\nfrom applying judgment is\n\n\n\n\n\n\n\n_i_ ( _[vR]_ [ +][ (1] _v_ ) _S_ )\n\n\n\n\n\n_t_ =2\n\n( _vR_ + (1 _v_ ) _S_ ).\n\n\n\nThe decision maker eventually can learn what to do and will earn a higher\npayoff than without judgment, but will trade this off against a delay in the\npayoff .\nThis calculation presumes that the decision maker knows the state—that\n\n- _i_ is true—prior to engaging in judgment. If this is not the case, then the\nexpected present discounted payoff to judgment on, say, �1 alone is\n\n\n\n1\n\n\n## (max v { μ( R + ( 1 μ) vR ( + ( 1 v ) r )) + ( 1 v ) μ( r + ( 1 μ) vR ( + ( 1 v ) r )), S })\n\n\n\n\n## (max v { μ( R + (1 μ) vR ( + (1 v ) r )), S } + (1 v ) S ),\n\n\n\nwhere the last step follows from equation (A1). To make exposition simpler,\nwe suppose that �1 =�2 = �. In addition, let [ˆ] = / 1– (1–( ) ); [ˆ] can be\ngiven a similar interpretation to �, the quality of judgment.\nIf the strategy were to apply judgment on one state only and then make\na decision, this would be the relevant payoff to consider. However, because\njudgment is possible in both states, there are several cases to consider.\nFirst, the decision maker might apply judgment to both states in sequence.\nIn this case, the expected present discounted payoff is\n\n\nYou are reading copyrighted material published by University of Chicago Press.\nUnauthorized posting, copying, or distributing of this work except as permitted under\nU.S. copyright law is illegal and injures the author and publisher.\n\n\nA Theory of Decision- Making and Artifi cial Intelligence **99**\n\n## ˆ [2] ( v [2] R + v (1 v )max μ{ R + (1 μ) r, S } + v (1 v )max μ{ r + (1 μ) R, S } + (1 v ) [2] S )\n\n= [ˆ][ 2] ( _v_ [2] _R_ + (1 _v_ [2] ) _S_ ),\n\n\nwhere the last step follows from equation (A1).\nSecond, the decision maker might apply judgment to, say, �1 fi rst and then,\ncontingent on the outcome there, apply judgment to �2. If the decision maker\nchooses to pursue judgment on �2 if the outcome for �1 is that the risky action\nis optimal, the payoff becomes\n\nˆ( _v_ ˆ _vR_ ( + (1 _v_ )max μ{ _R_ + (1 μ) _r_, _S_ })\n\n+ (1 _v_ )max μ{ _r_ + (1 μ) _vR_ ( + (1 _v_ ) _r_ ), _S_ })\n\n= [ˆ] ( _v_ [ˆ] ( _vR_ + (1 _v_ ) _S_ ) + (1 _v_ ) _S_ ).\n\n\nIf the decision maker chooses to pursue judgment on �2 after determining\nthat the outcome for �1 is that the safe action is optimal, the payoff becomes\n\nˆ( _v_ max μ{ _R_ + (1 μ) _vR_ ( + (1 _v_ ) _r_ ), _S_ }\n\n+ (1 _v_ ) [ˆ] ( _v_ max μ{ _r_ + (1 μ) _R_, _S_ } + (1 _v_ ) _S_ ))\n## = [ˆ] ( v max μ{ R + (1 μ) vR ( + (1 v ) r ), S } + (1 v ) [ˆ] S ).\n\n\nNote that this is option is dominated by not applying further judgment at\nall if the outcome for �1 is that the safe action is optimal.\nGiven this we can prove the following:\n\n\nProposition 1: _Under (A1) and (A2), and in the absence of any signal_\n_about the state, (a) judging both states and (b) continuing after the discovery_\n_that the safe action is preferred in a state are never optimal._\n\n\nProof: Note that judging two states is optimal if\n\n\n_S_\nˆ >\n_v_ max μ{ _r_ + (1 μ) _R_, _S_ } [+][ (1] _v_ ) _S_\n\n\n\nμ _R_ + (1 μ) _vR_ ( + (1 _v_ ) _r_ )\nˆ > ~~.~~\n\n_vR_ + (1 _v_ )max μ{ _R_ + (1 μ) _r_, _S_ }\n\n\n\nAs (A2) implies that � _r_ + (1 – �) _R_ ≤ _S_, the fi rst condition reduces to\nˆ\n - 1. Thus, (a) judging two states is dominated by judging one state and\ncontinuing to explore only if the risk is found to be optimal in that state.\nTurning to the strategy of continuing to apply judgment only if the\nsafe action is found to be preferred in a state, we can compare this to the\npayoff from applying judgment to one state and then acting immediately.\nNote that\n\n\nYou are reading copyrighted material published by University of Chicago Press.\nUnauthorized posting, copying, or distributing of this work except as permitted under\nU.S. copyright law is illegal and injures the author and publisher.\n\n\n**100  Ajay Agrawal, Joshua Gans, and Avi Goldfarb**\n\n## ˆ v ( max μ{ R + (1 μ) vR ( + (1 v ) r ), S } + (1 v ) [ˆ] S ) > [ˆ] ( v max μ{ R + (1 μ) vR ( + (1 v ) r ), S } + (1 v ) S ) [.]\n\n\nThis can never hold, proving that (b) is dominated.\n\n\nThe intuition is similar to Propositions 1 and 2 in Bolton and FaureGrimaud (2009). In particular, applying judgment is only useful if it is going\nto lead to the decision maker switching to the risky action. Thus, it is never\nworthwhile to unconditionally explore a second state as it may not change\nthe action taken. Similarly, if judging one state leads to knowledge the safe\naction continues to be optimal in that state, in the presence of uncertainty\nabout the state, even if knowledge is gained of the payoff to the risky action\nin the second state, that action will never be chosen. Hence, further judgment\nis not worthwhile. Hence, it is better to choose immediately at that point\nrather than delay the inevitable.\nGiven this proposition, there are only two strategies that are potentially\noptimal (in the absence of prediction). One strategy (we will term here J1)\nis where judgment is applied to one state and if the risky action is optimal,\nthen that action is taken immediately; otherwise, the safe default is taken\nimmediately. The state where judgment is applied fi rst is the state most likely\nto arise. This will be state 1 if � > 1/ 2. This strategy might be chosen if\n## ˆ v ( max μ{ R + (1 μ) vR ( + (1 v ) r ), S } + (1 v ) S ) > S\n\n\n\nˆ > ˆ\n\n\n\n_J_ 1\n\n\n\n_S_\n_v_ max μ{ _R_ + (1 μ) _vR_ ( + (1 _v_ ) _r_ ), _S_ } [+][ (1] _v_ ) _S_\n\n\n\n,\n\n\n\nwhich clearly requires that � _R_ + (1 – �)( _vR_ + (1 – _v_ ) _r_ ) > S.\nThe other strategy (we will term here J2) is where judgment is applied to\none state and if the risky action is optimal, then judgment is applied to the\nnext state; otherwise, the safe default is taken immediately. Note that J2 is\npreferred to J1 if\n\nˆ _v_ ( ˆ _vR_ ( + (1 _v_ ) _S_ ) + (1 _v_ ) _S_ )\n## > [ˆ] ( v max μ{ R + (1 μ) vR ( + (1 v ) r ), S } + (1 v ) S )\n\nˆ _v vR_ ( + (1 _v_ ) _S_ ) > _v_ max μ{ _R_ + (1 μ) _vR_ ( + (1 _v_ ) _r_ ), _S_ }\n\nmax μ{ _R_ + (1 μ) _vR_ ( + (1 _v_ ) _r_ ), _S_ }\nˆ > ~~.~~\n\n_vR_ + (1 _v_ ) _S_\n\n\nThis is intuitive. Basically, it is only when the effi ciency of judgment is suffi ciently high that more judgment is applied. However, for this inequality to\nbe relevant, J2 must also be preferred to the status quo yielding a payoff of\n_S_ . Thus, J2 is not dominated if\n\n\nYou are reading copyrighted material published by University of Chicago Press.\nUnauthorized posting, copying, or distributing of this work except as permitted under\nU.S. copyright law is illegal and injures the author and publisher.\n\n\nA Theory of Decision- Making and Artifi cial Intelligence **101**\n\n\n\n\n\n,\n\n\n\n,\n\n\n\nwhere the fi rst term is the range where J2 dominates J1, while the second\nterm is where J2 dominates _S_ alone; so for J2 to be optimal, it must exceed\nboth. Note also that as � → ( _S_ - _r_ )/ ( _R_ - _r_ ) (its highest possible level consistent\nwith [A1] and [A2]), then [ˆ] _J_ 2 [→] [ 1.]\n\nIf � _R_ + (1 – �)( _vR_ + (1 – _v_ ) _r_ ) > S, note that\n\n\n\n_S_\n\n\n_v_ μ( _R_ + (1 μ)( _vR_ + (1 _v_ ) _r_ ))+ (1 _v_ ) _S_\n\n\n\nˆ\n\n_J_ 2 [>][ ˆ]\n\n\n\n_J_ 1\n\n\n\nμ _R_ + (1 μ) _vR_ ( + (1 _v_ ) _r_ )\n\n\n_vR_ + (1 _v_ ) _S_\n\n\n## (1 v ) S μ( R + (1 μ) vR ( + (1 v ) r ) S )> v RS ( (μ R + (1 μ)( vR + (1 v ) r ))2) [,]\n\n\n\nwhich may not hold for _v_ suffi ciently high. However, it can be shown that\nwhen [ˆ] _J_ 2 [ + ] [ˆ] _J_ 1 [, then the two terms of ] [ˆ] _J_ 2 [ are equal and the second term ]\n\nexceeds the fi rst when [ˆ] _J_ 2 [ˆ] _J_ 1 [. This implies that in the range where ][ˆ] _J_ 2 [ < ][ˆ] _J_ 1 [, ]\n\nJ2 dominates J1.\nThis analysis implies there are two types of regimes with judgment only.\nIf [ˆ] [ > ] [ˆ] [, then easier decisions (with high ] [ˆ][) involve using J2, the next ]\n\n\n\n_J_ 2 [ + ] [ˆ]\n\n\n\n_J_ 1 [, then the two terms of ] [ˆ]\n\n\n\n_J_ 2 [ˆ]\n\n\n\n_J_ 2 [ < ][ˆ]\n\n\n\n_J_ 1 [. This implies that in the range where ][ˆ]\n\n\n\nIf [ˆ] _J_ 2 [ > ] [ˆ] _J_ 1 [, then easier decisions (with high ] [ˆ][) involve using J2, the next ]\n\ntranche of decisions use J1 (with intermediate [ˆ] ) while the remainder\nin volves no exercise of judgment at all. On the other hand, if [ˆ] _J_ 2 [ < ][ˆ] _J_ 1 [, then ]\n\nthe easier decisions involve using J2 while the remainder do not involve\njudgment at all.\n\n\n\n_J_ 2 [ > ] [ˆ]\n\n\n\n_J_ 2 [ < ][ˆ]\n\n\n\n3.4.2 Prediction in the Absence of Judgment\n\n\nNext, we consider the model with prediction but no judgment. Suppose\nthat there exists an AI that can, if deployed, identify the state prior to a\ndecision being made. In other words, prediction, if it occurs, is perfect; an\nassumption we will relax in a later section. Initially, suppose there is no\njudgment mechanism to determine what the optimal action is in each state.\nRecall that, in the absence of prediction or judgment, (A1) ensures that\nthe safe action will be chosen. If the decision maker knows the state, then\nthe risky action in a given state is chosen if\n\n\n_vR_ + (1 – _v_ ) _r_           - S.\n\n\nThis contradicts (A1). Thus, the expected payoff is\n\n\n_VP_ = _S_,\n\n\nwhich is the same outcome if there is no judgment or prediction.\n\n\n3.4.3 Prediction and Judgment Together\n\n\nBoth prediction and judgment can be valuable on their own. The question\nwe next wish to consider is whether they are complements or substitutes.\nWhile perfect prediction allows you to choose an action based on the\n\n\nYou are reading copyrighted material published by University of Chicago Press.\nUnauthorized posting, copying, or distributing of this work except as permitted under\nU.S. copyright law is illegal and injures the author and publisher.\n\n\n**102  Ajay Agrawal, Joshua Gans, and Avi Goldfarb**\n\n\nactual rather than expected state, it also aff ords the same opportunity with\nrespect to judgment. As judgment is costly, it is useful not to waste considering what action might be taken in a state that does not arise. This was\nnot possible when there was no prediction. But if you receive a prediction\nregarding the state, you can then apply judgment exclusively to actions in\nrelation to that state. To be sure, that judgment still involves a cost, but at\nthe same time does not lead to any wasted cognitive resources.\nGiven this, if the decision maker were the apply judgment after the state\nis predicted, their expected discounted payoff would be\n\n_VPJ_ = max { [ˆ] ( _vR_ + (1 _v_ ) _S_ ), _S_ }.\n\n\nThis represents the highest expected payoff possible (net of the costs of\njudgment). A necessary condition for both prediction and judgment to be\noptimal is that: [ˆ] ≥ [ˆ] [≡] _[s]_ [/ [] _[vR]_ [ + (1 – ] _[v]_ [)] _[S]_ []. Note that ][ˆ] [ ≤ ][ˆ] [, ][ˆ] [.]\n\n\n\n_PJ_ [≡] _[s]_ [/ [] _[vR]_ [ + (1 – ] _[v]_ [)] _[S]_ []. Note that ][ˆ]\n\n\n\n_PJ_ [ ≤ ][ˆ]\n\n\n\n_J_ 1 [, ][ˆ]\n\n\n\n_J_ 2 [.]\n\n\n\n3.4.4 Complements or Substitutes?\n\n\nTo evaluate whether prediction and judgment are complements or substitutes, we adopt the following parameterization for the eff ectiveness of\nprediction: we assume that with probability _e_ an AI yields a prediction, while\notherwise, the decision must be made in its absence (with judgment only).\nWith this parameterization, we can prove the following:\n\nProposition 2: _In the range of_  - _where_ [ˆ] _<_ [ˆ] _J2_ _[, e and ]_ [�] _[ are complements, ]_\n\n_otherwise they are substitutes._\n\n\n\nProof: Step 1. Is [ˆ] _J_ 2 [ > ] _[R]_ [/ [2(] _[vR]_ [ + (1 – ] _[v]_ [)] _[S]_ [)]? First, note that]\n\n\n\nmax μ{ _R_ + (1 μ)( _vR_ + (1 _v_ ) _r_ ), _S_ } _R_\n\n              _vR_ + (1 _v_ ) _S_ 2 _vR_ ( + (1 _v_ ) _S_ )\n\n\n## max μ{ R + (1 μ) vR ( + (1 v ) r ) [,] [S] } > [1]\n\n2 _[R]_ [.]\n\n\nNote that by (A2) and since � > (1/ 2), _S_  - � _R_ + (1 – �) _r_  - (1/ 2) _R_ so this\ninequality always holds.\nSecond, note that\n\n\n                  2 _v vR_ ( + (1 _v_ ) _S_ ) 2 _vR_ ( + (1 _v_ ) _S_ )\n\n_S_ 4( _v_ [2] _R_ + _S_ (1 + 2 _v_ 3 _v_ [2] )) > _vR_ ( + (1 _v_ ) _S_ )2\n\n\n_S_ ( _S_ 2 _R_ ) > _v_ ( _R_ [2] 6 _RS_ + _S_ [2] ),\n\n\nwhich holds as the left- hand side is always positive while the right- hand side\nis always negative.\nStep 2: Suppose that �R + (1 – �)( _vR_ + (1 – _v_ ) _r_ ) ≤ _S_ ; then J1 is never\noptimal. In this case, the expected payoff is\n\n\nYou are reading copyrighted material published by University of Chicago Press.\nUnauthorized posting, copying, or distributing of this work except as permitted under\nU.S. copyright law is illegal and injures the author and publisher.\n\n\nA Theory of Decision- Making and Artifi cial Intelligence **103**\n\n\n_eVPJ_ + (1 _e_ ) _VJ_ 2 = _e_ [ˆ] ( _vR_ + (1 _v_ ) _S_ ) + (1 _e_ ) [ˆ] ( _v_ [ˆ] ( _vR_ + (1 _v_ ) _S_ ) + (1 _v_ ) _S_ ) [.]\n\nThis mixed partial derivative with respect to (e, [ˆ] ) is _v_ ( _R_  - 2 [ˆ] ( _vR_ +\n(1 – _v_ ) _S_ )). This is positive if _R_ / [2( _vR_ + (1 – _v_ ) _S_ )] ≥ [ˆ] . By Step 1, this implies\nthat for [ˆ] < [ˆ] _J_ 2 [, prediction and judgment are complements; otherwise, they ]\n\nare substitutes.\n\n\nStep 3: Suppose that that � _R_ + (1 – �)( _vR_ + (1 – _v_ ) _r_ ) > _S_ . Note that for\nˆ _J_ 1 ˆ < ˆ _J_ 2, J1 is preferred to J2. In this case, the expected payoff to prediction\nand judgment is\n\n_e_ [ˆ] ( _vR_ + (1 _v_ ) _S_ )+ (1 _e_ ) [ˆ] ( _v_ max{ μ _R_ + (1 μ) _vR_ ( + (1 _v_ ) _r_ ), _S_ } + (1 _v_ ) _S_ ).\n\nThis mixed partial derivative with respect to (e, [ˆ] ) is _v_ ( _R_ - max{� _R_ +\n(1 – �)( _vR_ + (1 – _v_ ) _r_ ), _S_ }) > 0. By Step 1, this implies that for [ˆ] < [ˆ] _J_ 2 [, predic-]\n\ntion and judgment are complements; otherwise, they are substitutes.\n\nThe intuition is as follows. When [ˆ] < [ˆ] _J_ 2 [, then, in the absence of prediction ]\n\neither no judgment is applied or, alternatively, strategy J1 (with one round\nof judgment) is optimal; _e_ parameterizes the degree of diff erence between\nthe expected value with both prediction and judgment and the expected\nvalue without prediction with an increase in �, increasing both. However,\nwith one round of judgment, the increase when judgment is used alone is\nless than that when both are used together. Thus, when [ˆ] < [ˆ] _J_ 2 [, prediction ]\n\nand judgment are complements.\nBy contrast, when [ˆ]  - [ˆ] _J_ 2 [, then strategy J2 (with two rounds of judgment) ]\n\nis used in the absence of prediction. In this case, increasing � increases the\nexpected payoff from judgment alone disproportionately more because judgment is applied on both states, whereas under prediction and judgment it\nis only applied on one. Thus, improving the quality of judgment reduces\nthe returns to prediction. And so, when [ˆ] - [ˆ] _J_ 2 [, prediction and judgment are ]\n\nsubstitutes.\n\n\n**3.5 Complexity**\n\n\nThus far, the model illustrates the interplay between knowing the reward\nfunction (judgment) and prediction. While those results show that prediction and judgment can be substitutes, there is a sense in which they are\nmore naturally complements. The reason is this: what prediction enables is a\nform of state- contingent decision- making. Without a prediction, a decision\nmaker is forced to make the same choice regardless of the state that might\narise. In the spirit of Herbert Simon, one might call this a heuristic. And in\nthe absence of prediction, the role of judgment is to make that choice. Moreover, that choice is easier—that is, more likely to be optimal—when there\nexists dominant (or “near dominant”) choices. Thus, when either the state\nspace or the action space expand (as it may in more complex situations), it is\n\n\nYou are reading copyrighted material published by University of Chicago Press.\nUnauthorized posting, copying, or distributing of this work except as permitted under\nU.S. copyright law is illegal and injures the author and publisher.\n\n\n**104  Ajay Agrawal, Joshua Gans, and Avi Goldfarb**\n\n\nless likely that there will exist a dominant choice. In that regard, faced with\ncomplexity, in the absence of prediction, the value of judgment diminishes\nand we are more likely to see decision makers choose default actions that,\non average, are likely to be better than others.\nSuppose now we add a prediction machine to the mix. While in our\nmodel such a machine, when it renders a prediction, can perfectly signal\nthe state that will arise, let us consider a more convenient alternative that\nmay arise in complex situations: the prediction machine can perfectly signal\nsome states (should they arise), but for other states no precise prediction is\npossible except for the fact that one of those states is the correct one. In\nother words, the prediction machine can sometimes render a fi ne prediction\nand otherwise a coarse one. Here, an improvement in the prediction machine\nmeans an increase in the number of states in which the machine can render\na fi ne prediction.\nThus, consider an _N_  - state model where the probability of state _i_ is � _i_ .\nSuppose that states {1, . . ., _m_ } can be fi nely predicted by an AI, while the\nremainder cannot be distinguished. Suppose that in the states that cannot\nbe distinguished applying judgment is not worthwhile so that the optimal\nchoice is the safe action. Also, assume that when a prediction is available,\njudgment is worthwhile; that is, [ˆ] ≥ _s_ / [ _vR_ + (1 – _v_ ) _S_ ]. In this situation, the\nexpected present discounted value when both prediction and judgment are\navailable is\n\n\n\n\n\n_m_\n\n\n_i_ =1\n\n\n\nμ _i vR_ ( + (1 _v_ ) _S_ ) +\n\n\n\n_N_\n\nμ _iS_ .\n\n_i_ = _m_ +1\n\n\n\nSimilarly, it is easy to see that _VP_ = _VJ_ = _S_ = _V_ 0 as _vR_ + (1 – _v_ ) _r_ ≤ _S_ . Note\nthat as _m_ increases (perhaps because the prediction machine learns to predict\nmore states), then the marginal value of better judgment increases. That is,\n\nˆ μ _m vR_ ( + (1 _v_ ) _S_ ) μ _mS_ is increasing in [ˆ] .\nWhat happens as the situation becomes more complex (that is, _N_ increases)? An increase in _N_ will weakly lead to a reduction in � _i_ for any given\n_i_ . Holding _m_ fi xed (and so the quality of the prediction machine does not\nimprove with the complexity of the world), this will reduce the value of prediction and judgment as greater weight is placed on states where prediction\nis unavailable; that is, it is assumed that the increase in complexity does not,\nceteris paribus, create a state where prediction is available. Thus, complexity\nappears to be associated with _lower_ returns to both prediction and judgment. Put diff erently, an improvement in prediction machines would mean\n_m_ increases with _N_ fi xed. In this case, the returns to judgment rise as greater\nweight is put on states where prediction is available.\nThis insight is useful because there are several places in the economics\nliterature where complexity has interacted with other economic decisions.\nThese include automation, contracting, and fi rm boundaries. We discuss\neach of these in turn, highlighting potential implications.\n\n\nYou are reading copyrighted material published by University of Chicago Press.\nUnauthorized posting, copying, or distributing of this work except as permitted under\nU.S. copyright law is illegal and injures the author and publisher.\n\n\nA Theory of Decision- Making and Artifi cial Intelligence **105**\n\n\n3.5.1 Automation\n\n\nThe literature on automation is sometimes synonymous with AI. This\narises because AI may power new robots that are able to operate in open\nenvironments thanks to machine learning. For instance, while automated\ntrains have been possible for some time since they run on tracks, automated\ncars are new because they need to operate in far more complex environments.\nIt is prediction in those open environments that has allowed the emergence\nof environmentally fl exible capital equipment. Note that leads to the implication that as AI improves, tasks in more complex environments can be\nhandled by machines (Acemoglu and Restrepo 2017).\nHowever, this story masks the message that emerges from our analysis that\nrecent AI developments are all about prediction. Why prediction enables\nautomated vehicles is because it is relatively straightforward to describe (and\nhence, program) what those vehicles should do in diff erent situations. In\nother words, if prediction enables “state contingent decisions,” then automated vehicles arise because someone knows what decision is optimal in\neach state. In other words, automation means that judgment can be encoded\nin machine behavior. Prediction added to that means that automated capital\ncan be moved into more complex environments. In that respect, it is perhaps\nnatural to suggest that improvements in AI will lead to a substitution of\nhumans for machines as more tasks in more complex environments become\ncapable of being programmed in a state- contingent manner.\nThat said, there is another dimension of substitution that arises in complex environments. As noted above, when states cannot be predicted (something that for a given technology is more likely to be the case in more complex\nenvironments), then the actions chosen are more likely to be defaults or the\nresults of heuristics that perform, on average, well. Many, including Acemoglu and Restrepo (2017), argue that it is for more complex tasks that humans\nhave a comparative advantage relative to machines. However, this is not at\nall obvious. If it is known that a particular default or heuristic should be\nused, then a machine can be programmed to undertake this. In this regard,\nthe most complex tasks—precisely because little is known regarding how\nto take better actions given that the prediction of the state is coarse—may\nbe more, not less, amenable to automation.\nIf we had to speculate, imagine that states were ordered in terms of diminished likelihood (i.e., � _i_ ≥ � _j_ for all _i_ < _j_ ). The lowest index states might be\nones that, because they arrive frequently, there is knowledge of what the\noptimal action is in each and so they can be programmed to be handled by a\nmachine. The highest index states similarly, because the optimal action that\ncannot be determined can also be programmed. It is the intermediate states\nthat arise less frequently but not infrequently where, if a reliable prediction\nexisted, could be handled by humans applying judgment when those states\narose. Thus, the payoff could be written\n\n\nYou are reading copyrighted material published by University of Chicago Press.\nUnauthorized posting, copying, or distributing of this work except as permitted under\nU.S. copyright law is illegal and injures the author and publisher.\n\n\n**106  Ajay Agrawal, Joshua Gans, and Avi Goldfarb**\n\n\n\n_N_\n\nμ _iS_,\n\n_i_ = _m_ +1\n\n\n\n_m_\n\nμ _i vR_ ( + (1 _v_ ) _S_ ) +\n\n_i_ = _k_ +1\n\n\n\n_VPJ_ =\n\n\n\n_k_\n\n\n_i_ =1\n\n\n\n\n\nwhere tasks 1 through _k_ are automated using prediction because there is\nknowledge of the optimal action. If this was the matching of tasks to\nmachines and humans, then it is not at all clear whether an increase in complexity would be associated with more or less human employment.\nThat said, the issue for the automation literature is not subtleties over\nthe term “complex tasks,” but as AI becomes more prevalent, where might\nthe substitution of machines for humans arise. As noted above, an increase\nin AI increases _m_ . At this margin, humans are able to come into the marginal tasks and, because a prediction machine is available, use judgment to\nconduct state- contingent decisions in those situations. Absent other eff ects,\ntherefore, an increase in AI is associated with more human labor on any\ngiven task. However, as the weight on those marginal tasks is falling in the\nlevel of complexity, it may not be the more complex tasks that humans are\nperforming more of. On the other hand, one can imagine that in a model\nwith a full labor market equilibrium that an increase in AI that enables\nmore human judgment at the margin may also create opportunities to study\nthat judgment to see if it can be programmed into lower index states and\nbe handled by machines. So, while the AI does not necessarily cause more\nroutine tasks to be handled by machines, it might create the economic conditions that lead to just that.\n\n\n3.5.2 Contracting\n\n\nContracting shares much with programming. Here is Jean Tirole (2009,\n265) on the subject:\n\n\nIts general thrust goes as follows. The parties to a contract (buyer, seller)\ninitially avail themselves of an available design, perhaps an industry standard. This design or contract is the best contract under existing knowledge. The parties are unaware, however, of the contract’s implications, but\nthey realize that something may go wrong with this contract; indeed, they\nmay exert cognitive eff ort in order to fi nd out about what may go wrong\nand how to draft the contract accordingly: put diff erently, a _contingency_\nis foreseeable (perhaps at a prohibitively high cost), but not necessarily\nforeseen. To take a trivial example, the possibility that the price of oil\nincreases, implying that the contract should be indexed on it, is perfectly\nforeseeable, but this does not imply that parties will think about this possibility and index the contract price accordingly.\n\n\nTirole argues that contingencies can be planned for in contracts using cognitive eff ort (akin to what we have termed here as judgment), while others may\nbe optimally left out because the eff ort is too costly relative to the return\ngiven, say, the low likelihood that contingency arises.\nThis logic can assist us in understanding what prediction machines might\n\n\nYou are reading copyrighted material published by University of Chicago Press.\nUnauthorized posting, copying, or distributing of this work except as permitted under\nU.S. copyright law is illegal and injures the author and publisher.\n\n\nA Theory of Decision- Making and Artifi cial Intelligence **107**\n\n\ndo to contracts. If an AI becomes available then, in writing contracts, it is\npossible, because fi ne state predictions are possible, to incur cognitive costs\nto determine what the contingencies should be if those states should arise.\nFor other states, the contract will be left incomplete—perhaps for a default\naction or alternatively some renegotiation process. A direct implication of\nthis is that contracts may well become less incomplete.\nOf course, when it comes to employment contracts, the eff ects may be\ndiff erent. As Herbert Simon (1951) noted, employment contracts diff er from\nother contracts precisely because it is often not possible to specify what\nactions should be performed in what circumstance. Hence, what those contracts often allocate are diff erent decision rights.\nWhat is of interest here is the notion that contacts can be specifi ed\nclearly—that is, programmed—but also that prediction can activate the\nuse of human judgment. That latter notion means that actions cannot be\neasily contracted—by defi nition, contractibility is programming and needing judgment implies that programming was not possible. Thus, as prediction machines improve and more human judgment is optimal, then that\njudgment will be applied outside of objective contract measures—including\nobjective performance measures. If we had to speculate, this would favor\nmore subjective performance processes, including relational contracts\n(Baker, Gibbons, and Murphy 1999). [9]\n\n\n3.5.3 Firm Boundaries\n\n\nWe now turn to consider what impact AI may have on fi rm boundaries\n(that is, the make or buy decision). Suppose that it is a buyer ( _B_ ) who receives\nthe value from a decision taken—that is, the payoff from the risky or safe\naction as the case may be. To make things simple, let’s assume that � _i_ = �\nfor all _i_, so that _V_ = _k vR_ ( + (1 _v_ ) _S_ ) + [ˆ] ( _m_ _k_ ) _vR_ ( + (1 _v_ ) _S_ ) + ( _N_ _m_ ) _S_ .\nWe suppose that the tasks are undertaken by a seller ( _S_ ). The tasks\n{1, . . ., _k_ } and { _m_ + 1, . . ., _N_ ) can be contracted upon, while the intermediate tasks require the seller to exercise judgment. We suppose that the\ncost of providing judgment is a function _c_ ( [ˆ] ), which is nondecreasing and\nconvex. (We write this function in terms of [ˆ] just to keep the notation\nsimple.) The costs can be anticipated by the buyer. So if one of the intermediate states arises, the buyer can choose to give the seller a fi xed price\ncontract (and bear none of the costs) or a cost- plus contract (and bear all\nof them).\nFollowing Tadelis (2002), we assume that the seller market is competitive\nand so all surplus accrues to the buyer. In this case, the buyer return is\n\n\n9. A recent paper by Dogan and Yildirim (2017) actually considers how automation might\nimpact on worker contracts. However, they do not examine AI per se, and focus on how it might\nchange objective performance measures in teams moving from joint performance evaluation\nto more relative performance evaluation.\n\n\nYou are reading copyrighted material published by University of Chicago Press.\nUnauthorized posting, copying, or distributing of this work except as permitted under\nU.S. copyright law is illegal and injures the author and publisher.\n\n\n**108  Ajay Agrawal, Joshua Gans, and Avi Goldfarb**\n\n## k vR ( + (1 v ) S ) + max { [ˆ] ( m k ) vR ( + (1 v ) S ), S } + ( N m ) S p zc [( )][ˆ],\n\nwhile the seller return is: _p_ - (1 – _z_ ) _c_ ( [ˆ] ). Here _p_ + _zc_ ( [ˆ] ) is the contract price\nand _z_ is 0 for a fi xed price contract and 1 for a cost- plus contract. Note that\nonly with a cost- plus contract does the seller exercise any judgment. Thus,\nthe buyer chooses a cost- plus over a fi xed price contract if\n\n_k vR_ ( + (1 _v_ ) _S_ ) + max { [ˆ] ( _m_ _k_ ) _vR_ ( + (1 _v_ ) _S_ ), _S_ } [+][ (] _[N]_ _m_ ) _S_ _c_ [( )][ˆ]\n\n         - _k vR_ ( + (1 _v_ ) _S_ ) + ( _N_ _k_ ) _S_ .\n\n\nIt is easy to see that as _m_ rises (i.e., prediction becomes cheaper), a cost- plus\ncontract is more likely to be chosen. That is, incentives fall as prediction\nbecomes more abundant.\nNow we can consider the impact of integration. We assume that the buyer\ncan choose to make the decisions themselves, but at a higher cost. That is,\n_c_ ( [ˆ], _I_ ) > _c_ ( [ˆ] ) where _I_ denotes integration. We also assume that ∂ _c_ ( [ˆ], _I_ )/ ∂ [ˆ] \n( _c_ ( [ˆ] ) / ˆ ). Under integration, the buyer’s value is\n\n_k vR_ ( + (1 _v_ ) _S_ ) + [ˆ][ *] ( _m_ _k_ ) _vR_ ( + (1 _v_ ) _S_ ) + ( _N_ _m_ ) _S_ _c_ ( [ˆ][ *], _I_ )\n\nwhere [ˆ][ *] maximizes the buyer payoff in this case. Given this, it can easily be\nseen that as _m_ increases, the returns to integration rise.\nBy contrast, notice that as _k_ increases, the incentives for a cost- plus contract are diminished and the returns to integration fall. Thus, the more prediction machines allow for the placement of contingencies in a contract (the\nlarger _m- k_ ), the higher powered will seller incentives be and the more likely\nthere is to be integration.\nForbes and Lederman (2009) showed that airlines are more likely to vertically integrate with regional partners when scheduling is more complex:\nspecifi cally, where bad weather is more likely to lead to delays. The impact of\nprediction machines will depend on whether they lead to an increase in the\nnumber of states where the action can be automated in a state- contingent\nmanner ( _k_ ) relative to the increase in the number of states where the state\nbecomes known but the action cannot be automated ( _m_ ). If the former, then\nwe will see more vertical integration with the rise of prediction machines. If\nthe latter, we will see less. The diff erence is driven by the need for more costly\njudgment in the vertically integrated case as _m- k_ rises.\n\n\n**3.6 Conclusions**\n\n\nIn this chapter, we explore the consequences of recent improvements in\nmachine- learning technology that have advanced the broader fi eld of artifi cial intelligence. In particular, we argue that these advances in the ability of\nmachines to conduct mental tasks are driven by improvements in machine\nprediction. In order to understand how improvements in machine prediction\nwill impact decision- making, it is important to analyze how the payoff s of\nthe model arise. We label the process of learning payoff s “judgment.”\n\n\nYou are reading copyrighted material published by University of Chicago Press.\nUnauthorized posting, copying, or distributing of this work except as permitted under\nU.S. copyright law is illegal and injures the author and publisher.\n\n\nA Theory of Decision- Making and Artifi cial Intelligence **109**\n\n\nBy modeling judgment explicitly, we derive a number of useful insights\ninto the value of prediction. We show that prediction and judgment are generally complements, as long as judgment is not too diffi cult. We also show\nthat improvements in judgment change the type of prediction quality that\nis most useful: better judgment means that more accurate predictions are\nvaluable relative to more frequent predictions. Finally, we explore the role of\ncomplexity, demonstrating that, in the presence of complexity, the impact of\nimproved prediction on the value of judgment depends on whether improved\nprediction leads to automated decision- making. Complexity is a key aspect\nof economic research in automation, contracting, and the boundaries of\nthe fi rm. As prediction machines improve, our model suggests that the consequences in complex environments are particularly fruitful to study.\nThere are numerous directions research in this area could proceed. First,\nthe chapter does not explicitly model the form of the prediction—including what measures might be the basis for decision- making. In reality, this\nis an important design variable and impacts on the accuracy of predictions and decision- making. In computer science, this is referred to as the\nchoice of surrogates, and this appears to be a topic amenable for economic\ntheoretical investigation. Second, the chapter treats judgment as largely a\nhuman- directed activity. However, we have noted that it can else be encoded,\nbut have not been explicit about the process by which this occurs. Endogenising this—perhaps relating it to the accumulation of experience—would be\nan avenue for further investigation. Finally, this is a single- agent model. It\nwould be interesting to explore how judgment and prediction mix when each\nis impacted upon by the actions and decisions of other agents in a game\ntheoretic setting.\n\n\n**References**\n\n\nAcemoglu, Daron. 2003. “Labor- and Capital- Augmenting Technical Change.”\n_Journal of the European Economic Association_ 1 (1): 1– 37.\nAcemoglu, Daron, and Pascual Restrepo. 2017. “The Race between Machine and\nMan: Implications of Technology for Growth, Factor Shares, and Employment.”\nWorking paper, Massachusetts Institute of Technology.\nAgrawal, Ajay, Joshua S. Gans, and Avi Goldfarb. 2018a. “Human Judgment and\nAI Pricing.” _American Economic Association: Papers & Proceedings_, 108:58–63.\n———. 2018b. _Prediction Machines: The Simple Economics of Artifi cial Intelligence._\nBoston, MA: Harvard Business Review Press.\nAlpaydin, Ethem. 2010. _Introduction to Machine Learning_, 2nd ed. Cambridge, MA:\nMIT Press.\nAutor, David. 2015. “Why Are There Still So Many Jobs? The History and Future of\nWorkplace Automation.” _Journal of Economic Perspectives_ 29 (3): 3– 30.\nBaker, George, Robert Gibbons, and Kevin Murphy. 1999. “Informal Authority in\nOrganizations.” _Journal of Law, Economics, and Organization_ 15:56– 73.\nBelloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2014. “High\n\nYou are reading copyrighted material published by University of Chicago Press.\nUnauthorized posting, copying, or distributing of this work except as permitted under\nU.S. copyright law is illegal and injures the author and publisher.\n\n\n**110  Andrea Prat**\n\n\nDimensional Methods and Inference on Structural and Treatment Eff ects.” _Jour-_\n_nal of Economic Perspectives_ 28 (2): 29– 50.\nBenzell, Seth G., Laurence J. Kotlikoff, Guillermo LaGarda, and Jeff rey D. Sachs.\n2015. “Robots Are Us: Some Economics of Human Replacement.” NBER Working Paper no. 20941, Cambridge, MA.\nBolton, P., and A. Faure- Grimaud. 2009. “Thinking Ahead: The Decision Problem.”\n_Review of Economic Studies_ 76:1205– 38.\nBrynjolfsson, Erik, and Andrew McAfee. 2014. _The Second Machine Age_ . New York:\nW. W. Norton.\nDogan, M., and P. Yildirim. 2017. “Man vs. Machine: When Is Automation Inferior\nto Human Labor?” Unpublished manuscript, The Wharton School of the University of Pennsylvania.\nDomingos, Pedro. 2015. _The Master Algorithm._ New York: Basic Books.\nForbes, Silke, and Mara Lederman. 2009. “Adaptation and Vertical Integration in\nthe Airline Industry.” _American Economic Review_ 99 (5): 1831– 49.\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. _The Elements of_\n_Statistical Learning: Data Mining, Inference, and Prediction_, 2nd ed. New York:\nSpringer.\nHawkins, Jeff . 2004. _On Intelligence_ . New York: Times Books.\nJha, S., and E. J. Topol. 2016. “Adapting to Artifi cial Intelligence: Radiologists and\nPathologists as Information Specialists.” _Journal of the American Medical Associa-_\n_tion_ 316 (22): 2353– 54.\nLusted, L. B. 1960. “Logical Analysis in Roentgen Diagnosis.” _Radiology_ 74:178– 93.\nMarkov, John. 2015. _Machines of Loving Grace_ . New York: HarperCollins Publishers.\nNg, Andrew. 2016. “What Artifi cial Intelligence Can and Can’t Do Right Now.”\n_Harvard Business Review Online_ . Accessed Dec. 8, 2016. https:// hbr .org/ 2016/ 11\n/ what- artifi cial- intelligence- can- and- cant- do- right- now.\nSimon, H. A. 1951. “A Formal Theory of the Employment Relationship.” _Econo-_\n_metrica_ 19 (3): 293– 305.\nTadelis, S. 2002. “Complexity, Flexibility and the Make- or- Buy Decision.” _American_\n_Economic Review_ 92 (2): 433– 37.\nTirole, J. 2009. “Cognition and Incomplete Contracts.” _American Economic Review_\n99 (1): 265– 94.\nVarian, Hal R. 2014. “Big Data: New Tricks for Econometrics.” _Journal of Economic_\n_Perspectives_ 28 (2): 3– 28.\n\n\n**Comment** Andrea Prat\n\n\nOne of the key activities of organizations is to collect, process, combine,\nand utilize information (Arrow 1974). A modern corporation exploits\nthe vast amounts of data that it accumulates from marketing, operations,\nhuman resources, fi nance, and other functions to grow faster and be more\n\n\nAndrea Prat is the Richard Paul Richman Professor of Business at Columbia Business\nSchool and professor of economics at Columbia University.\nFor acknowledgments, sources of research support, and disclosure of the author’s material\nfi nancial relationships, if any, please see http:// www .nber .org/ chapters/ c14022.ack.\n\n\nYou are reading copyrighted material published by University of Chicago Press.\nUnauthorized posting, copying, or distributing of this work except as permitted under\nU.S. copyright law is illegal and injures the author and publisher.\n\n\n",
  "raw_text_norm": "This PDF is a selection from a published volume from the National Bureau of Economic Research Volume Title: The Economics of Artificial Intelligence: An Agenda Volume Authors/Editors: Ajay Agrawal, Joshua Gans, and Avi Goldfarb, editors Volume Publisher: University of Chicago Press Volume ISBNs: 978-0-226-61333-8 (cloth); 978-0-226-61347-5 (electronic) Volume URL: http://www.nber.org/books/agra-1 Conference Date: September 13–14, 2017 Publication Date: May 2019 Chapter Title: Prediction, Judgment, and Complexity: A Theory of Decision-Making and Artificial Intelligence Chapter Author(s): Ajay Agrawal, Joshua Gans, Avi Goldfarb Chapter URL: http://www.nber.org/chapters/c14010 Chapter pages in book: (p. 89 – 110) # **and Complexity** A Theory of Decision- Making and Artifi cial Intelligence Ajay Agrawal, Joshua Gans, and Avi Goldfarb **3.1 Introduction** There is widespread discussion regarding the impact of machines on employment (see Autor 2015). In some sense, the discussion mirrors a longstanding literature on the impact of the accumulation of capital equipment on employment; specifi cally, whether capital and labor are substitutes or complements (Acemoglu 2003). But the recent discussion is motivated by the integration of software with hardware and whether the role of machines goes beyond physical tasks to mental ones as well (Brynjolfsson and McAfee 2014). As mental tasks were seen as always being present and essential, human comparative advantage in these was seen as the main reason why, at least in the long term, capital accumulation would complement employment by enhancing labor productivity in those tasks. The computer revolution has blurred the line between physical and men Ajay Agrawal is the Peter Munk Professor of Entrepreneurship at the Rotman School of Management, University of Toronto, and a research associate of the National Bureau of Economic Research. Joshua Gans is professor of strategic management and holder of the Jeff rey S. Skoll Chair of Technical Innovation and Entrepreneurship at the Rotman School of Management, University of Toronto (with a cross appointment in the Department of Economics), and a research associate of the National Bureau of Economic Research. Avi Goldfarb holds the Rotman Chair in Artifi cial Intelligence and Healthcare and is professor of marketing at the Rotman School of Management, University of Toronto, and is a research associate of the National Bureau of Economic Research. Our thanks to Andrea Prat, Scott Stern, Hal Varian, and participants at the AEA (Chicago), NBER Summer Institute (2017), NBER Economics of AI Conference (Toronto), Columbia Law School, Harvard Business School, MIT, and University of Toronto for helpful comments. Responsibility for all errors remains our own. The latest version of this chapter is available at joshuagans .com. For acknowledgments, sources of research support, and disclosure of the authors’ material fi nancial relationships, if any, please see http:// www .nber .org/ chapters / c14010.ack. You are reading copyrighted material published by University of Chicago Press. **89** Unauthorized posting, copying, or distributing of this work except as permitted under U.S. copyright law is illegal and injures the author and publisher. **90 Ajay Agrawal, Joshua Gans, and Avi Goldfarb** tal tasks. For instance, the invention of the spreadsheet in the late 1970s fundamentally changed the role of bookkeepers. Prior to that invention, there was a time- intensive task involving the recomputation of outcomes in spreadsheets as data or assumptions changed. That human task was substituted by the spreadsheet software that could produce the calculations more quickly, cheaply, and frequently. However, at the same time, the spreadsheet made the jobs of accountants, analysts, and others far more productive. In the accounting books, capital was substituting for labor, but the mental productivity of labor was being changed. Thus, the impact on employment critically depended on whether there were tasks the “computers cannot do.” These assumptions persist in models today. Acemoglu and Restrepo (2017) observe that capital substitutes for labor in certain tasks while at the same time technological progress creates new tasks. They make what they call a “natural assumption” that only labor can perform the new tasks as they are more complex than previous ones. [1] Benzell et al. (2015) consider the impact of software more explicitly. Their environment has two types of labor—high- tech (who can, among other things, code) and low- tech (who are empathetic and can handle interpersonal tasks). In this environment, it is the low- tech workers who cannot be replaced by machines while the high- tech ones are employed initially to create the code that will eventually displace their kind. The results of the model depend, therefore, on a class of worker who cannot be substituted directly for capital, but also on the inability of workers themselves to substitute between classes. In this chapter, our approach is to delve into the weeds of what is happening currently in the fi eld of artifi cial intelligence (AI). The recent wave of developments in AI all involve advances in machine learning. Those advances allow for automated and cheap prediction; that is, providing a forecast (or nowcast) of a variable of interest from available data (Agrawal, Gans and Goldfarb 2018b). In some cases, prediction has enabled full automation of tasks—for example, self- driving vehicles where the process of data collection, prediction of behavior and surroundings, and actions are all conducted without a human in the loop. In other cases, prediction is a standalone tool—such as image recognition or fraud detection—that may or may not lead to further substitution of human users of such tools by machines. Thus far, substitution between humans and machines has focused mainly on cost considerations. Are machines cheaper, more reliable, and more scalable (in their software form) than humans? This chapter, however, considers the role of prediction in decision- making explicitly and from that examines the complementary skills that may be matched with prediction within a task. 1. To be sure, their model is designed to examine how automation of tasks causes a change in factor prices that biases innovation toward the creation of new tasks that labor is more suited to. You are reading copyrighted material published by University of Chicago Press. Unauthorized posting, copying, or distributing of this work except as permitted under U.S. copyright law is illegal and injures the author and publisher. A Theory of Decision- Making and Artifi cial Intelligence **91** Our focus, in this regard, is on what we term _judgment_ . While judgment is a term with broad meaning, here we use it to refer to a very specifi c skill. To see this, consider a decision. That decision involves choosing an action, _x_, from a set, _X_ . The payoff (or reward) from that action is defi ned by a function, _u_ ( _x_, �) where � is a realization of an uncertain state drawn from a distribution, _F_ (�). Suppose that, prior to making a decision, a _prediction_ (or signal), _s_, can be generated that results in a posterior, _F_ (�| _s_ ). Thus, the decision maker would solve In other words, a standard problem of choice under uncertainty. In this standard world, the role of prediction is to improve decision- making. The payoff, or utility function, is known. To create a role for judgment, we depart from this standard set-up in statistical decision theory and ask how a decision maker comes to know the function, _u_ ( _x_, �)? We assume that this is not simply given or a primitive of the decision- making model. Instead, it requires a human to undertake a costly process that allows the mapping from ( _x_, �) to a particular payoff value, _u_, to be discovered. This is a reasonable assumption given that beyond some rudimentary experimentation in closed environments, there is no current way for an AI to impute a utility function that resides with humans. Additionally, this process separates the costs of providing the mapping for each pair, ( _x_, �). (Actually, we focus, without loss in generality, on situations where _u_ ( _x_, �) ≠ _u_ ( _x_ ) for all � and presume that if a payoff to an action is state independent that payoff is known.) In other words, while prediction can obtain a signal of the underlying state, judgment is the process by which the payoff s from actions that arise based on that state can be determined. We assume that this process of determining payoff s requires human understanding of the situation: it is not a prediction problem. For intuition on the diff erence between prediction and judgment, consider the example of credit card fraud. A bank observes a credit card transaction. That transaction is either legitimate or fraudulent. The decision is whether to approve the transaction. If the bank knows for sure that the transaction is legitimate, the bank will approve it. If the bank knows for sure that it is fraudulent, the bank will refuse the transaction. Why? Because the bank knows the payoff of approving a legitimate transaction is higher than the payoff of refusing that transaction. Things get more interesting if the bank is uncertain about whether the transaction is legitimate. The uncertainty means that the bank also needs to know the payoff from refusing a legitimate transaction and from approving a fraudulent transaction. In our model, judgment is the process of determining these payoff s. It is a costly activity, in the sense that it requires time and eff ort. As the new developments regarding AI all involve making prediction more readily available, we ask, how does judgment and its endogenous appli You are reading copyrighted material published by University of Chicago Press. Unauthorized posting, copying, or distributing of this work except as permitted under U.S. copyright law is illegal and injures the author and publisher. **92 Ajay Agrawal, Joshua Gans, and Avi Goldfarb** cation change the value of prediction? Are prediction and judgment substitutes or complements? How does the value of prediction change monotonically with the diffi culty of applying judgment? In complex environments (as they relate to automation, contracting, and the boundaries of the fi rm), how do improvements in prediction aff ect the value of judgment? We proceed by fi rst providing supportive evidence for our assumption that recent developments in AI overwhelmingly impact the costs of prediction. We then use the example of radiology to provide a context for understanding the diff erent roles of prediction and judgment. Drawing inspiration from Bolton and Faure- Grimaud (2009), we then build the baseline model with two states of the world and uncertainty about payoff s to actions in each state. We explore the value of judgment in the absence of any prediction technology, and then the value of prediction technology when there is no judgment. We fi nish the discussion of the baseline model with an exploration of the interaction between prediction and judgment, demonstrating that prediction and judgment are complements as long as judgment isn’t too diffi cult. We then separate prediction quality into prediction frequency and prediction accuracy. As judgment improves, accuracy becomes more important relative to frequency. Finally, we examine complex environments where the number of potential states is large. Such environments are common in economic models of automation, contracting, and boundaries of the fi rm. We show that the eff ect of improvements in prediction on the importance of judgment depend a great deal on whether the improvements in prediction enable automated decision- making. **3.2 AI and Prediction Costs** We argue that the recent advances in artifi cial intelligence are advances in the technology of prediction. Most broadly, we defi ne prediction as the ability to take known information to generate new information. Our model emphasizes prediction about the state of the world. Most contemporary artifi cial intelligence research and applications come from a fi eld now called “machine learning.” Many of the tools of machine learning have a long history in statistics and data analysis, and are likely familiar to economists and applied statisticians as tools for prediction and classifi cation. [2] For example, Alpaydin’s (2010) textbook _Introduction to_ _Machine Learning_ covers maximum likelihood estimation, Bayesian estimation, multivariate linear regression, principal components analysis, clustering, and nonparametric regression. In addition, it covers tools that may be less familiar, but also use independent variables to predict outcomes: 2. We defi ne prediction as known information to generate new information. Therefore, classifi cation techniques such as clustering are prediction techniques in which the new information to be predicted is the appropriate category or class. You are reading copyrighted material published by University of Chicago Press. Unauthorized posting, copying, or distributing of this work except as permitted under U.S. copyright law is illegal and injures the author and publisher. A Theory of Decision- Making and Artifi cial Intelligence **93** regression trees, neural networks, hidden Markov models, and reinforcement learning. Hastie, Tibshirani, and Friedman (2009) cover similar topics. The 2014 _Journal of Economic Perspectives_ symposium on big data covered several of these less familiar prediction techniques in articles by Varian (2014) and Belloni, Chernozhukov, and Hansen (2014). While many of these prediction techniques are not new, recent advances in computer speed, data collection, data storage, and the prediction methods themselves have led to substantial improvements. These improvements have transformed the computer science research fi eld of artifi cial intelligence. The Oxford English Dictionary defi nes artifi cial intelligence as “[t]he theory and development of computer systems able to perform tasks normally requiring human intelligence.” In the 1960s and 1970s, artifi cial intelligence research was primarily rules- based, symbolic logic. It involved human experts generating rules that an algorithm could follow (Domingos 2015, 89). These are not prediction technologies. Such systems became very good chess players and they guided factory robots in highly controlled settings; however, by the 1980s, it became clear that rules- based systems could not deal with the complexity of many nonartifi cial settings. This led to an “AI winter” in which research funding artifi cial intelligence projects largely dried up (Markov 2015). Over the past ten years, a diff erent approach to artifi cial intelligence has taken off . The idea is to program computers to “learn” from example data or experience. In the absence of the ability to predetermine the decision rules, a data- driven prediction approach can conduct many mental tasks. For example, humans are good at recognizing familiar faces, but we would struggle to explain and codify this skill. By connecting data on names to image data on faces, machine learning solves this problem by predicting which image data patterns are associated with which names. As a prominent artifi cial intelligence researcher put it, “Almost all of AI’s recent progress is through one type, in which some input data (A) is used to quickly generate some simple response (B)” (Ng 2016). Thus, the progress is explicitly about improvements in prediction. In other words, the suite of technologies that have given rise to the recent resurgence of interest in artifi cial intelligence use data collected from sensors, images, videos, typed notes, or anything else that can be represented in bits to fi ll in missing information, recognize objects, or forecast what will happen next. To be clear, we do not take a position on whether these prediction technologies really do mimic the core aspects of human intelligence. While Palm Computing founder Jeff Hawkins argues that human intelligence is—in essence—prediction (Hawkins 2004), many neuroscientists, psychologists, and others disagree. Our point is that the technologies that have been given the label artifi cial intelligence are prediction technologies. Therefore, in order to understand the impact of these technologies, it is important to assess the impact of prediction on decisions. You are reading copyrighted material published by University of Chicago Press. Unauthorized posting, copying, or distributing of this work except as permitted under U.S. copyright law is illegal and injures the author and publisher. **94 Ajay Agrawal, Joshua Gans, and Avi Goldfarb** **3.3 Case: Radiology** Before proceeding to the model, we provide some intuition of how prediction and judgment apply in a particular context where prediction machines are expected to have a large impact: radiology. In 2016, Geoff Hinton—one of the pioneers of deep learning neural networks—stated that it was no longer worth training radiologists. His strong implication was that radiologists would not have a future. This is something that radiologists have been concerned about since 1960 (Lusted 1960). Today, machine- learning techniques are being heavily applied in radiology by IBM using its Watson computer and by a start-up, Enlitic. Enlitic has been able to use deep learning to detect lung nodules (a fairly routine exercise) [3] but also fractures (which is more complex). Watson can now identify pulmonary embolism and some other heart issues. These advances are at the heart of Hinton’s forecast, but have also been widely discussed among radiologists and pathologists (Jha and Topol 2016). What does the model in this chapter suggest about the future of radiologists? If we consider a simplifi ed characterization of the job of a radiologist, it would be that they examine an image in order to characterize and classify that image and return an assessment to a physician. While often that assessment is a diagnosis (i.e., “the patient has pneumonia”), in many cases, the assessment is in the negative (i.e., “pneumonia not excluded”). In that regard, this is stated as a predictive task to inform the physician of the likelihood of the state of the world. Using that, the physician can devise a treatment. These predictions are what machines are aiming to provide. In particular, it might provide a diff erential diagnosis of the following kind: _Based on Mr Patel’s demographics and imaging, the mass in the liver has a_ _66.6 percent chance of being benign, 33.3 percent chance of being malignant,_ _and a 0.1 percent of not being real._ [4] The action is whether some intervention is needed. For instance, if a potential tumor is identifi ed in a noninvasive scan, then this will inform whether an invasive examination will be conducted. In terms of identifying the state of the world, the invasive exam is costly but safe—it can deduce a cancer with certainty and remove it if necessary. The role of a noninvasive exam is to inform whether an invasive exam should be forgone. That is, it is to make physicians more confi dent about abstaining from treatment and further analysis. In this regard, if the machine improves prediction, it will lead to fewer invasive examinations. 3. “You did not go to medical school to measure lung nodules.” http:// www .medscape .com / viewarticle/ 863127#vp_2. 4. http:// www .medscape .com/ viewarticle/ 863127#vp_3. You are reading copyrighted material published by University of Chicago Press. Unauthorized posting, copying, or distributing of this work except as permitted under U.S. copyright law is illegal and injures the author and publisher. A Theory of Decision- Making and Artifi cial Intelligence **95** Judgment involves understanding the payoff s. What is the payoff to conducting a biopsy if the mass is benign, malignant, or not real? What is the payoff to not doing anything in those three states? The issue for radiologists in particular is whether a trained specialist radiologist is in the best position to make this judgment or will it occur further along the chain of decisionmaking or involve new job classes that merge diagnostic information such as a combined radiologist/ pathologist (Jha and Topol 2016). Next, we formalize these ideas. **3.4 Baseline Model** Our baseline model is inspired by the “bandit” environment considered by Bolton and Faure- Grimaud (2009), although it departs signifi cantly in the questions addressed and base assumptions made. Like them, in our baseline model, we suppose there are two states of the world, {�1,�2} with prior probabilities of {�,1 – �}. There are two possible actions: a state independent action with known payoff of _S_ (safe) and a state dependent action with two possible payoff s, _R_ or _r,_ as the case may be (risky). As noted in the introduction, a key departure from the usual assumptions of rational decision- making is that the decision maker does not know the payoff from the risky action in each state and must apply _judgment_ to determine that payoff . [5] Moreover, decision makers need to be able to make a judgment for each state that might arise in order to formulate a plan that would be the equivalent of payoff maximization. In the absence of such judgment, the ex ante expectation that the risky action is optimal in any state is _v_ (which is independent between states). To make things more concrete, we assume _R_ - _S_ - _r_ . [6] Thus, we assume that _v_ is the probability in any state that the risky payoff is _R_ rather than _r_ . This is not a conditional probability of the state. It is a statement about the payoff, given the state. In the absence of knowledge regarding the specifi c payoff s from the risky action, a decision can only be made on the basis of prior probabilities. Then the safe action will be chosen if μ _vR_ ( + (1 _v_ ) _r_ ) + 1( μ) _vR_ ( + (1 _v_ ) _r_ ) = _vR_ + (1 _v_ ) _r_ _S_ . 5. Bolton and Faure- Grimaud (2009) consider this step to be the equivalent of a thought experiment where thinking takes time. To the extent that our results can be interpreted as a statement about the comparative advantage of humans, we assume that only humans can do judgment. 6. Thus, we assume that the payoff function, _u_, can only take one of three values, { _R_, _r_, _S_ }. The issue is which combinations of state realization and action lead to which payoff s. However, we assume that _S_ is the payoff from the safe action regardless of state and so this is known to the decision maker. As it is the relative payoff s from actions that drive the results, this assumption is without loss in generality. Requiring this property of the safe action to be discovered would just add an extra cost. Implicitly, as the decision maker cannot make a decision in complete ignorance, we are assuming that the safe action’s payoff can be judged at an arbitrarily low cost. You are reading copyrighted material published by University of Chicago Press. Unauthorized posting, copying, or distributing of this work except as permitted under U.S. copyright law is illegal and injures the author and publisher. **96 Ajay Agrawal, Joshua Gans, and Avi Goldfarb** So that the payoff is: _V_ 0 = max{ _vR_ + (1 – _v_ ) _r_, _S_ }. To make things simpler, we will focus our attention on the case where the safe action is—in the absence of prediction or judgment—the default. That is, we assume that (A1) **(Safe Default)** _vR_ + (1 – _v_ ) _r_ ≤ _S_ . This assumption is made for simplicity only and will not change the qualitative conclusions. [7] Under (A1), in the absence of knowledge of the payoff function or a signal of the state, the decision maker would choose _S_ . 3.4.1 Judgment in the Absence of Prediction Prediction provides knowledge of the state. The process of judgment provides knowledge of the payoff function. Judgment therefore allows the decision maker to understand which action is optimal for a given state should it arise. Suppose that this knowledge is gained without cost (as it would be assumed to do under the usual assumptions of economic rationality). In other words, the decision maker has knowledge of optimal action in a given state. Then the risky action will be chosen (a) if it is the preferred action in both states (which arises with probability _v_ [2] ); (b) if it is the preferred action in �1 but not �2 and � _R_ + (1 – �) _r_ - _S_ (with probability _v_ (1 – _v_ )); or (c) if it is the preferred action in �2 but not �1 and � _r_ + (1 – �) _R_ - _S_ (with probability _v_ (1 – _v_ )). Thus, the expected payoff is ## v [2] R + v (1 v )max μ{ R + (1 μ) r, S } + v (1 v )max μ{ r + (1 μ) R, S } [+][ (1] v ) [2] S . Note that this is greater than _V_ 0. The reason for this is that, when there is uncertainty, judgment is valuable because it can identify actions that are dominant or dominated—that is, that might be optimal across states. In this situation, any resolution of uncertainty does not matter as it will not change the decision made. A key insight is that judgment itself can be consequential. Result 1: _If_ max{� _R_ + (1 – �) _r,_ - _r_ + (1 – �) _R_ } > _S, it is possible that_ _judgment alone can cause the decision to switch from the default action (safe)_ _to the alternative action (risky)._ As we are motivated by understanding the interplay between prediction and judgment, we want to make these consequential. Therefore, we make the following assumption to ensure prediction always has some value: (A2) **(Judgment Insuffi cient)** max{� _R_ + (1 – �) _r,_ - _r_ + (1 – �) _R_ } ≤ _S_ . Under this assumption, if diff erent actions are optimal in each state and this is known, the decision maker will not change to the risky action. This, of course, implies that the expected payoff is 7. Bolton and Faure- Grimaud (2009) make the opposite assumption. Here, as our focus is on the impact of prediction, it is better to consider environments where prediction has the eff ect of reducing uncertainty over riskier actions. You are reading copyrighted material published by University of Chicago Press. Unauthorized posting, copying, or distributing of this work except as permitted under U.S. copyright law is illegal and injures the author and publisher. A Theory of Decision- Making and Artifi cial Intelligence **97** _v_ [2] _R_ + (1 _v_ [2] ) _S_ . Note that, absent any cost, full judgment improves the decision maker’s expected payoff . Judgment does not come for free. We assume here that it takes time (although the formulation would naturally match with the notion that it takes costly eff ort). Suppose the discount factor is � < 1. A decision maker can spend time in a period determining what the optimal action is for a particular state. If they choose to apply judgment with respect to state � _i_, then there is a probability � _i_ that they will determine the optimal action in that period and can make a choice based on that judgment. Otherwise, they can choose to apply judgment to that problem in the next period. It is useful, at this point, to consider what judgment means once it has been applied. The initial assumption we make here is that the knowledge of the payoff function depreciates as soon as a decision is made. In other words, applying judgment can delay a decision (and that is costly) and it can improve that decision (which is its value) but it cannot generate experience that can be applied to other decisions (including future ones). In other words, the initial conception of judgment is the application of _thought_ rather than the gathering of _experience_ . [8] Practically, this reduces our examination to a static model. However, in a later section, we consider the experience formulation and demonstrate that most of the insights of the static model carry over to the dynamic model. In summary, the timing of the game is as follows: 1. At the beginning of a decision stage, the decision maker chooses whether to apply judgment and to what state or whether to simply choose an action without judgment. If an action is chosen, uncertainty is resolved and payoff s are realized and we move to a new decision stage. 2. If judgment is chosen, with probability, 1 – � _i_, they do not fi nd out the payoff s for the risky action in that state, a period of time elapses and the game moves back to 1. With probability � _i_, the decision maker gains this knowledge. The decision maker can then take an action, uncertainty is resolved and payoff s are realized, and we move to a new decision stage (back to 1). If no action is taken, a period of time elapses and the current decision stage continues. 3. The decision maker chooses whether to apply judgment to the other state. If an action is chosen, uncertainty is resolved and payoff s are realized and we move to a new decision stage (back to 1). 4. If judgment is chosen, with probability, 1 – �– _i_, they do not fi nd out the payoff s for the risky action in that state, a period of time elapses and the game moves back to 1. With probability �– _i_, the decision maker gains this knowledge. The decision maker then chooses an action, uncertainty 8. The experience frame is considered in Agrawal, Gans, and Goldfarb (2018a). You are reading copyrighted material published by University of Chicago Press. Unauthorized posting, copying, or distributing of this work except as permitted under U.S. copyright law is illegal and injures the author and publisher. **98 Ajay Agrawal, Joshua Gans, and Avi Goldfarb** **Table 3.1** **Model parameters** Parameter Description _S_ Known payoff from the safe action _R_ Potential payoff from the risky action in a given state _r_ Potential payoff from the risky action in a given state θ _i_ Label of state _i_ ∈ {1,2} - Probability of state 1 _v_ Prior probability that the payoff in a given state is _R_ λ _i_ Probablilty that decision maker learns the payoff to the risky action θ _i_ if judgment is applied for one period δ Discount factor is resolved and payoff s are realized, and we move to a new decision stage (back to 1). When prediction is available, it will become available prior to the beginning of a decision stage. The various parameters are listed in table 3.1. Suppose that the decision maker focuses on judging the optimal action (i.e., assessing the payoff ) for � _i_ . Then the expected present discount payoff from applying judgment is _i_ ( _[vR]_ [ +][ (1] _v_ ) _S_ ) _t_ =2 ( _vR_ + (1 _v_ ) _S_ ). The decision maker eventually can learn what to do and will earn a higher payoff than without judgment, but will trade this off against a delay in the payoff . This calculation presumes that the decision maker knows the state—that - _i_ is true—prior to engaging in judgment. If this is not the case, then the expected present discounted payoff to judgment on, say, �1 alone is 1 ## (max v { μ( R + ( 1 μ) vR ( + ( 1 v ) r )) + ( 1 v ) μ( r + ( 1 μ) vR ( + ( 1 v ) r )), S }) ## (max v { μ( R + (1 μ) vR ( + (1 v ) r )), S } + (1 v ) S ), where the last step follows from equation (A1). To make exposition simpler, we suppose that �1 =�2 = �. In addition, let [ˆ] = / 1– (1–( ) ); [ˆ] can be given a similar interpretation to �, the quality of judgment. If the strategy were to apply judgment on one state only and then make a decision, this would be the relevant payoff to consider. However, because judgment is possible in both states, there are several cases to consider. First, the decision maker might apply judgment to both states in sequence. In this case, the expected present discounted payoff is You are reading copyrighted material published by University of Chicago Press. Unauthorized posting, copying, or distributing of this work except as permitted under U.S. copyright law is illegal and injures the author and publisher. A Theory of Decision- Making and Artifi cial Intelligence **99** ## ˆ [2] ( v [2] R + v (1 v )max μ{ R + (1 μ) r, S } + v (1 v )max μ{ r + (1 μ) R, S } + (1 v ) [2] S ) = [ˆ][ 2] ( _v_ [2] _R_ + (1 _v_ [2] ) _S_ ), where the last step follows from equation (A1). Second, the decision maker might apply judgment to, say, �1 fi rst and then, contingent on the outcome there, apply judgment to �2. If the decision maker chooses to pursue judgment on �2 if the outcome for �1 is that the risky action is optimal, the payoff becomes ˆ( _v_ ˆ _vR_ ( + (1 _v_ )max μ{ _R_ + (1 μ) _r_, _S_ }) + (1 _v_ )max μ{ _r_ + (1 μ) _vR_ ( + (1 _v_ ) _r_ ), _S_ }) = [ˆ] ( _v_ [ˆ] ( _vR_ + (1 _v_ ) _S_ ) + (1 _v_ ) _S_ ). If the decision maker chooses to pursue judgment on �2 after determining that the outcome for �1 is that the safe action is optimal, the payoff becomes ˆ( _v_ max μ{ _R_ + (1 μ) _vR_ ( + (1 _v_ ) _r_ ), _S_ } + (1 _v_ ) [ˆ] ( _v_ max μ{ _r_ + (1 μ) _R_, _S_ } + (1 _v_ ) _S_ )) ## = [ˆ] ( v max μ{ R + (1 μ) vR ( + (1 v ) r ), S } + (1 v ) [ˆ] S ). Note that this is option is dominated by not applying further judgment at all if the outcome for �1 is that the safe action is optimal. Given this we can prove the following: Proposition 1: _Under (A1) and (A2), and in the absence of any signal_ _about the state, (a) judging both states and (b) continuing after the discovery_ _that the safe action is preferred in a state are never optimal._ Proof: Note that judging two states is optimal if _S_ ˆ > _v_ max μ{ _r_ + (1 μ) _R_, _S_ } [+][ (1] _v_ ) _S_ μ _R_ + (1 μ) _vR_ ( + (1 _v_ ) _r_ ) ˆ > ~~.~~ _vR_ + (1 _v_ )max μ{ _R_ + (1 μ) _r_, _S_ } As (A2) implies that � _r_ + (1 – �) _R_ ≤ _S_, the fi rst condition reduces to ˆ - 1. Thus, (a) judging two states is dominated by judging one state and continuing to explore only if the risk is found to be optimal in that state. Turning to the strategy of continuing to apply judgment only if the safe action is found to be preferred in a state, we can compare this to the payoff from applying judgment to one state and then acting immediately. Note that You are reading copyrighted material published by University of Chicago Press. Unauthorized posting, copying, or distributing of this work except as permitted under U.S. copyright law is illegal and injures the author and publisher. **100 Ajay Agrawal, Joshua Gans, and Avi Goldfarb** ## ˆ v ( max μ{ R + (1 μ) vR ( + (1 v ) r ), S } + (1 v ) [ˆ] S ) > [ˆ] ( v max μ{ R + (1 μ) vR ( + (1 v ) r ), S } + (1 v ) S ) [.] This can never hold, proving that (b) is dominated. The intuition is similar to Propositions 1 and 2 in Bolton and FaureGrimaud (2009). In particular, applying judgment is only useful if it is going to lead to the decision maker switching to the risky action. Thus, it is never worthwhile to unconditionally explore a second state as it may not change the action taken. Similarly, if judging one state leads to knowledge the safe action continues to be optimal in that state, in the presence of uncertainty about the state, even if knowledge is gained of the payoff to the risky action in the second state, that action will never be chosen. Hence, further judgment is not worthwhile. Hence, it is better to choose immediately at that point rather than delay the inevitable. Given this proposition, there are only two strategies that are potentially optimal (in the absence of prediction). One strategy (we will term here J1) is where judgment is applied to one state and if the risky action is optimal, then that action is taken immediately; otherwise, the safe default is taken immediately. The state where judgment is applied fi rst is the state most likely to arise. This will be state 1 if � > 1/ 2. This strategy might be chosen if ## ˆ v ( max μ{ R + (1 μ) vR ( + (1 v ) r ), S } + (1 v ) S ) > S ˆ > ˆ _J_ 1 _S_ _v_ max μ{ _R_ + (1 μ) _vR_ ( + (1 _v_ ) _r_ ), _S_ } [+][ (1] _v_ ) _S_ , which clearly requires that � _R_ + (1 – �)( _vR_ + (1 – _v_ ) _r_ ) > S. The other strategy (we will term here J2) is where judgment is applied to one state and if the risky action is optimal, then judgment is applied to the next state; otherwise, the safe default is taken immediately. Note that J2 is preferred to J1 if ˆ _v_ ( ˆ _vR_ ( + (1 _v_ ) _S_ ) + (1 _v_ ) _S_ ) ## > [ˆ] ( v max μ{ R + (1 μ) vR ( + (1 v ) r ), S } + (1 v ) S ) ˆ _v vR_ ( + (1 _v_ ) _S_ ) > _v_ max μ{ _R_ + (1 μ) _vR_ ( + (1 _v_ ) _r_ ), _S_ } max μ{ _R_ + (1 μ) _vR_ ( + (1 _v_ ) _r_ ), _S_ } ˆ > ~~.~~ _vR_ + (1 _v_ ) _S_ This is intuitive. Basically, it is only when the effi ciency of judgment is suffi ciently high that more judgment is applied. However, for this inequality to be relevant, J2 must also be preferred to the status quo yielding a payoff of _S_ . Thus, J2 is not dominated if You are reading copyrighted material published by University of Chicago Press. Unauthorized posting, copying, or distributing of this work except as permitted under U.S. copyright law is illegal and injures the author and publisher. A Theory of Decision- Making and Artifi cial Intelligence **101** , , where the fi rst term is the range where J2 dominates J1, while the second term is where J2 dominates _S_ alone; so for J2 to be optimal, it must exceed both. Note also that as � → ( _S_ - _r_ )/ ( _R_ - _r_ ) (its highest possible level consistent with [A1] and [A2]), then [ˆ] _J_ 2 [→] [ 1.] If � _R_ + (1 – �)( _vR_ + (1 – _v_ ) _r_ ) > S, note that _S_ _v_ μ( _R_ + (1 μ)( _vR_ + (1 _v_ ) _r_ ))+ (1 _v_ ) _S_ ˆ _J_ 2 [>][ ˆ] _J_ 1 μ _R_ + (1 μ) _vR_ ( + (1 _v_ ) _r_ ) _vR_ + (1 _v_ ) _S_ ## (1 v ) S μ( R + (1 μ) vR ( + (1 v ) r ) S )> v RS ( (μ R + (1 μ)( vR + (1 v ) r ))2) [,] which may not hold for _v_ suffi ciently high. However, it can be shown that when [ˆ] _J_ 2 [ + ] [ˆ] _J_ 1 [, then the two terms of ] [ˆ] _J_ 2 [ are equal and the second term ] exceeds the fi rst when [ˆ] _J_ 2 [ˆ] _J_ 1 [. This implies that in the range where ][ˆ] _J_ 2 [ < ][ˆ] _J_ 1 [, ] J2 dominates J1. This analysis implies there are two types of regimes with judgment only. If [ˆ] [ > ] [ˆ] [, then easier decisions (with high ] [ˆ][) involve using J2, the next ] _J_ 2 [ + ] [ˆ] _J_ 1 [, then the two terms of ] [ˆ] _J_ 2 [ˆ] _J_ 2 [ < ][ˆ] _J_ 1 [. This implies that in the range where ][ˆ] If [ˆ] _J_ 2 [ > ] [ˆ] _J_ 1 [, then easier decisions (with high ] [ˆ][) involve using J2, the next ] tranche of decisions use J1 (with intermediate [ˆ] ) while the remainder in volves no exercise of judgment at all. On the other hand, if [ˆ] _J_ 2 [ < ][ˆ] _J_ 1 [, then ] the easier decisions involve using J2 while the remainder do not involve judgment at all. _J_ 2 [ > ] [ˆ] _J_ 2 [ < ][ˆ] 3.4.2 Prediction in the Absence of Judgment Next, we consider the model with prediction but no judgment. Suppose that there exists an AI that can, if deployed, identify the state prior to a decision being made. In other words, prediction, if it occurs, is perfect; an assumption we will relax in a later section. Initially, suppose there is no judgment mechanism to determine what the optimal action is in each state. Recall that, in the absence of prediction or judgment, (A1) ensures that the safe action will be chosen. If the decision maker knows the state, then the risky action in a given state is chosen if _vR_ + (1 – _v_ ) _r_ - S. This contradicts (A1). Thus, the expected payoff is _VP_ = _S_, which is the same outcome if there is no judgment or prediction. 3.4.3 Prediction and Judgment Together Both prediction and judgment can be valuable on their own. The question we next wish to consider is whether they are complements or substitutes. While perfect prediction allows you to choose an action based on the You are reading copyrighted material published by University of Chicago Press. Unauthorized posting, copying, or distributing of this work except as permitted under U.S. copyright law is illegal and injures the author and publisher. **102 Ajay Agrawal, Joshua Gans, and Avi Goldfarb** actual rather than expected state, it also aff ords the same opportunity with respect to judgment. As judgment is costly, it is useful not to waste considering what action might be taken in a state that does not arise. This was not possible when there was no prediction. But if you receive a prediction regarding the state, you can then apply judgment exclusively to actions in relation to that state. To be sure, that judgment still involves a cost, but at the same time does not lead to any wasted cognitive resources. Given this, if the decision maker were the apply judgment after the state is predicted, their expected discounted payoff would be _VPJ_ = max { [ˆ] ( _vR_ + (1 _v_ ) _S_ ), _S_ }. This represents the highest expected payoff possible (net of the costs of judgment). A necessary condition for both prediction and judgment to be optimal is that: [ˆ] ≥ [ˆ] [≡] _[s]_ [/ [] _[vR]_ [ + (1 – ] _[v]_ [)] _[S]_ []. Note that ][ˆ] [ ≤ ][ˆ] [, ][ˆ] [.] _PJ_ [≡] _[s]_ [/ [] _[vR]_ [ + (1 – ] _[v]_ [)] _[S]_ []. Note that ][ˆ] _PJ_ [ ≤ ][ˆ] _J_ 1 [, ][ˆ] _J_ 2 [.] 3.4.4 Complements or Substitutes? To evaluate whether prediction and judgment are complements or substitutes, we adopt the following parameterization for the eff ectiveness of prediction: we assume that with probability _e_ an AI yields a prediction, while otherwise, the decision must be made in its absence (with judgment only). With this parameterization, we can prove the following: Proposition 2: _In the range of_ - _where_ [ˆ] _<_ [ˆ] _J2_ _[, e and ]_ [�] _[ are complements, ]_ _otherwise they are substitutes._ Proof: Step 1. Is [ˆ] _J_ 2 [ > ] _[R]_ [/ [2(] _[vR]_ [ + (1 – ] _[v]_ [)] _[S]_ [)]? First, note that] max μ{ _R_ + (1 μ)( _vR_ + (1 _v_ ) _r_ ), _S_ } _R_ _vR_ + (1 _v_ ) _S_ 2 _vR_ ( + (1 _v_ ) _S_ ) ## max μ{ R + (1 μ) vR ( + (1 v ) r ) [,] [S] } > [1] 2 _[R]_ [.] Note that by (A2) and since � > (1/ 2), _S_ - � _R_ + (1 – �) _r_ - (1/ 2) _R_ so this inequality always holds. Second, note that 2 _v vR_ ( + (1 _v_ ) _S_ ) 2 _vR_ ( + (1 _v_ ) _S_ ) _S_ 4( _v_ [2] _R_ + _S_ (1 + 2 _v_ 3 _v_ [2] )) > _vR_ ( + (1 _v_ ) _S_ )2 _S_ ( _S_ 2 _R_ ) > _v_ ( _R_ [2] 6 _RS_ + _S_ [2] ), which holds as the left- hand side is always positive while the right- hand side is always negative. Step 2: Suppose that �R + (1 – �)( _vR_ + (1 – _v_ ) _r_ ) ≤ _S_ ; then J1 is never optimal. In this case, the expected payoff is You are reading copyrighted material published by University of Chicago Press. Unauthorized posting, copying, or distributing of this work except as permitted under U.S. copyright law is illegal and injures the author and publisher. A Theory of Decision- Making and Artifi cial Intelligence **103** _eVPJ_ + (1 _e_ ) _VJ_ 2 = _e_ [ˆ] ( _vR_ + (1 _v_ ) _S_ ) + (1 _e_ ) [ˆ] ( _v_ [ˆ] ( _vR_ + (1 _v_ ) _S_ ) + (1 _v_ ) _S_ ) [.] This mixed partial derivative with respect to (e, [ˆ] ) is _v_ ( _R_ - 2 [ˆ] ( _vR_ + (1 – _v_ ) _S_ )). This is positive if _R_ / [2( _vR_ + (1 – _v_ ) _S_ )] ≥ [ˆ] . By Step 1, this implies that for [ˆ] < [ˆ] _J_ 2 [, prediction and judgment are complements; otherwise, they ] are substitutes. Step 3: Suppose that that � _R_ + (1 – �)( _vR_ + (1 – _v_ ) _r_ ) > _S_ . Note that for ˆ _J_ 1 ˆ < ˆ _J_ 2, J1 is preferred to J2. In this case, the expected payoff to prediction and judgment is _e_ [ˆ] ( _vR_ + (1 _v_ ) _S_ )+ (1 _e_ ) [ˆ] ( _v_ max{ μ _R_ + (1 μ) _vR_ ( + (1 _v_ ) _r_ ), _S_ } + (1 _v_ ) _S_ ). This mixed partial derivative with respect to (e, [ˆ] ) is _v_ ( _R_ - max{� _R_ + (1 – �)( _vR_ + (1 – _v_ ) _r_ ), _S_ }) > 0. By Step 1, this implies that for [ˆ] < [ˆ] _J_ 2 [, predic-] tion and judgment are complements; otherwise, they are substitutes. The intuition is as follows. When [ˆ] < [ˆ] _J_ 2 [, then, in the absence of prediction ] either no judgment is applied or, alternatively, strategy J1 (with one round of judgment) is optimal; _e_ parameterizes the degree of diff erence between the expected value with both prediction and judgment and the expected value without prediction with an increase in �, increasing both. However, with one round of judgment, the increase when judgment is used alone is less than that when both are used together. Thus, when [ˆ] < [ˆ] _J_ 2 [, prediction ] and judgment are complements. By contrast, when [ˆ] - [ˆ] _J_ 2 [, then strategy J2 (with two rounds of judgment) ] is used in the absence of prediction. In this case, increasing � increases the expected payoff from judgment alone disproportionately more because judgment is applied on both states, whereas under prediction and judgment it is only applied on one. Thus, improving the quality of judgment reduces the returns to prediction. And so, when [ˆ] - [ˆ] _J_ 2 [, prediction and judgment are ] substitutes. **3.5 Complexity** Thus far, the model illustrates the interplay between knowing the reward function (judgment) and prediction. While those results show that prediction and judgment can be substitutes, there is a sense in which they are more naturally complements. The reason is this: what prediction enables is a form of state- contingent decision- making. Without a prediction, a decision maker is forced to make the same choice regardless of the state that might arise. In the spirit of Herbert Simon, one might call this a heuristic. And in the absence of prediction, the role of judgment is to make that choice. Moreover, that choice is easier—that is, more likely to be optimal—when there exists dominant (or “near dominant”) choices. Thus, when either the state space or the action space expand (as it may in more complex situations), it is You are reading copyrighted material published by University of Chicago Press. Unauthorized posting, copying, or distributing of this work except as permitted under U.S. copyright law is illegal and injures the author and publisher. **104 Ajay Agrawal, Joshua Gans, and Avi Goldfarb** less likely that there will exist a dominant choice. In that regard, faced with complexity, in the absence of prediction, the value of judgment diminishes and we are more likely to see decision makers choose default actions that, on average, are likely to be better than others. Suppose now we add a prediction machine to the mix. While in our model such a machine, when it renders a prediction, can perfectly signal the state that will arise, let us consider a more convenient alternative that may arise in complex situations: the prediction machine can perfectly signal some states (should they arise), but for other states no precise prediction is possible except for the fact that one of those states is the correct one. In other words, the prediction machine can sometimes render a fi ne prediction and otherwise a coarse one. Here, an improvement in the prediction machine means an increase in the number of states in which the machine can render a fi ne prediction. Thus, consider an _N_ - state model where the probability of state _i_ is � _i_ . Suppose that states {1, . . ., _m_ } can be fi nely predicted by an AI, while the remainder cannot be distinguished. Suppose that in the states that cannot be distinguished applying judgment is not worthwhile so that the optimal choice is the safe action. Also, assume that when a prediction is available, judgment is worthwhile; that is, [ˆ] ≥ _s_ / [ _vR_ + (1 – _v_ ) _S_ ]. In this situation, the expected present discounted value when both prediction and judgment are available is _m_ _i_ =1 μ _i vR_ ( + (1 _v_ ) _S_ ) + _N_ μ _iS_ . _i_ = _m_ +1 Similarly, it is easy to see that _VP_ = _VJ_ = _S_ = _V_ 0 as _vR_ + (1 – _v_ ) _r_ ≤ _S_ . Note that as _m_ increases (perhaps because the prediction machine learns to predict more states), then the marginal value of better judgment increases. That is, ˆ μ _m vR_ ( + (1 _v_ ) _S_ ) μ _mS_ is increasing in [ˆ] . What happens as the situation becomes more complex (that is, _N_ increases)? An increase in _N_ will weakly lead to a reduction in � _i_ for any given _i_ . Holding _m_ fi xed (and so the quality of the prediction machine does not improve with the complexity of the world), this will reduce the value of prediction and judgment as greater weight is placed on states where prediction is unavailable; that is, it is assumed that the increase in complexity does not, ceteris paribus, create a state where prediction is available. Thus, complexity appears to be associated with _lower_ returns to both prediction and judgment. Put diff erently, an improvement in prediction machines would mean _m_ increases with _N_ fi xed. In this case, the returns to judgment rise as greater weight is put on states where prediction is available. This insight is useful because there are several places in the economics literature where complexity has interacted with other economic decisions. These include automation, contracting, and fi rm boundaries. We discuss each of these in turn, highlighting potential implications. You are reading copyrighted material published by University of Chicago Press. Unauthorized posting, copying, or distributing of this work except as permitted under U.S. copyright law is illegal and injures the author and publisher. A Theory of Decision- Making and Artifi cial Intelligence **105** 3.5.1 Automation The literature on automation is sometimes synonymous with AI. This arises because AI may power new robots that are able to operate in open environments thanks to machine learning. For instance, while automated trains have been possible for some time since they run on tracks, automated cars are new because they need to operate in far more complex environments. It is prediction in those open environments that has allowed the emergence of environmentally fl exible capital equipment. Note that leads to the implication that as AI improves, tasks in more complex environments can be handled by machines (Acemoglu and Restrepo 2017). However, this story masks the message that emerges from our analysis that recent AI developments are all about prediction. Why prediction enables automated vehicles is because it is relatively straightforward to describe (and hence, program) what those vehicles should do in diff erent situations. In other words, if prediction enables “state contingent decisions,” then automated vehicles arise because someone knows what decision is optimal in each state. In other words, automation means that judgment can be encoded in machine behavior. Prediction added to that means that automated capital can be moved into more complex environments. In that respect, it is perhaps natural to suggest that improvements in AI will lead to a substitution of humans for machines as more tasks in more complex environments become capable of being programmed in a state- contingent manner. That said, there is another dimension of substitution that arises in complex environments. As noted above, when states cannot be predicted (something that for a given technology is more likely to be the case in more complex environments), then the actions chosen are more likely to be defaults or the results of heuristics that perform, on average, well. Many, including Acemoglu and Restrepo (2017), argue that it is for more complex tasks that humans have a comparative advantage relative to machines. However, this is not at all obvious. If it is known that a particular default or heuristic should be used, then a machine can be programmed to undertake this. In this regard, the most complex tasks—precisely because little is known regarding how to take better actions given that the prediction of the state is coarse—may be more, not less, amenable to automation. If we had to speculate, imagine that states were ordered in terms of diminished likelihood (i.e., � _i_ ≥ � _j_ for all _i_ < _j_ ). The lowest index states might be ones that, because they arrive frequently, there is knowledge of what the optimal action is in each and so they can be programmed to be handled by a machine. The highest index states similarly, because the optimal action that cannot be determined can also be programmed. It is the intermediate states that arise less frequently but not infrequently where, if a reliable prediction existed, could be handled by humans applying judgment when those states arose. Thus, the payoff could be written You are reading copyrighted material published by University of Chicago Press. Unauthorized posting, copying, or distributing of this work except as permitted under U.S. copyright law is illegal and injures the author and publisher. **106 Ajay Agrawal, Joshua Gans, and Avi Goldfarb** _N_ μ _iS_, _i_ = _m_ +1 _m_ μ _i vR_ ( + (1 _v_ ) _S_ ) + _i_ = _k_ +1 _VPJ_ = _k_ _i_ =1 where tasks 1 through _k_ are automated using prediction because there is knowledge of the optimal action. If this was the matching of tasks to machines and humans, then it is not at all clear whether an increase in complexity would be associated with more or less human employment. That said, the issue for the automation literature is not subtleties over the term “complex tasks,” but as AI becomes more prevalent, where might the substitution of machines for humans arise. As noted above, an increase in AI increases _m_ . At this margin, humans are able to come into the marginal tasks and, because a prediction machine is available, use judgment to conduct state- contingent decisions in those situations. Absent other eff ects, therefore, an increase in AI is associated with more human labor on any given task. However, as the weight on those marginal tasks is falling in the level of complexity, it may not be the more complex tasks that humans are performing more of. On the other hand, one can imagine that in a model with a full labor market equilibrium that an increase in AI that enables more human judgment at the margin may also create opportunities to study that judgment to see if it can be programmed into lower index states and be handled by machines. So, while the AI does not necessarily cause more routine tasks to be handled by machines, it might create the economic conditions that lead to just that. 3.5.2 Contracting Contracting shares much with programming. Here is Jean Tirole (2009, 265) on the subject: Its general thrust goes as follows. The parties to a contract (buyer, seller) initially avail themselves of an available design, perhaps an industry standard. This design or contract is the best contract under existing knowledge. The parties are unaware, however, of the contract’s implications, but they realize that something may go wrong with this contract; indeed, they may exert cognitive eff ort in order to fi nd out about what may go wrong and how to draft the contract accordingly: put diff erently, a _contingency_ is foreseeable (perhaps at a prohibitively high cost), but not necessarily foreseen. To take a trivial example, the possibility that the price of oil increases, implying that the contract should be indexed on it, is perfectly foreseeable, but this does not imply that parties will think about this possibility and index the contract price accordingly. Tirole argues that contingencies can be planned for in contracts using cognitive eff ort (akin to what we have termed here as judgment), while others may be optimally left out because the eff ort is too costly relative to the return given, say, the low likelihood that contingency arises. This logic can assist us in understanding what prediction machines might You are reading copyrighted material published by University of Chicago Press. Unauthorized posting, copying, or distributing of this work except as permitted under U.S. copyright law is illegal and injures the author and publisher. A Theory of Decision- Making and Artifi cial Intelligence **107** do to contracts. If an AI becomes available then, in writing contracts, it is possible, because fi ne state predictions are possible, to incur cognitive costs to determine what the contingencies should be if those states should arise. For other states, the contract will be left incomplete—perhaps for a default action or alternatively some renegotiation process. A direct implication of this is that contracts may well become less incomplete. Of course, when it comes to employment contracts, the eff ects may be diff erent. As Herbert Simon (1951) noted, employment contracts diff er from other contracts precisely because it is often not possible to specify what actions should be performed in what circumstance. Hence, what those contracts often allocate are diff erent decision rights. What is of interest here is the notion that contacts can be specifi ed clearly—that is, programmed—but also that prediction can activate the use of human judgment. That latter notion means that actions cannot be easily contracted—by defi nition, contractibility is programming and needing judgment implies that programming was not possible. Thus, as prediction machines improve and more human judgment is optimal, then that judgment will be applied outside of objective contract measures—including objective performance measures. If we had to speculate, this would favor more subjective performance processes, including relational contracts (Baker, Gibbons, and Murphy 1999). [9] 3.5.3 Firm Boundaries We now turn to consider what impact AI may have on fi rm boundaries (that is, the make or buy decision). Suppose that it is a buyer ( _B_ ) who receives the value from a decision taken—that is, the payoff from the risky or safe action as the case may be. To make things simple, let’s assume that � _i_ = � for all _i_, so that _V_ = _k vR_ ( + (1 _v_ ) _S_ ) + [ˆ] ( _m_ _k_ ) _vR_ ( + (1 _v_ ) _S_ ) + ( _N_ _m_ ) _S_ . We suppose that the tasks are undertaken by a seller ( _S_ ). The tasks {1, . . ., _k_ } and { _m_ + 1, . . ., _N_ ) can be contracted upon, while the intermediate tasks require the seller to exercise judgment. We suppose that the cost of providing judgment is a function _c_ ( [ˆ] ), which is nondecreasing and convex. (We write this function in terms of [ˆ] just to keep the notation simple.) The costs can be anticipated by the buyer. So if one of the intermediate states arises, the buyer can choose to give the seller a fi xed price contract (and bear none of the costs) or a cost- plus contract (and bear all of them). Following Tadelis (2002), we assume that the seller market is competitive and so all surplus accrues to the buyer. In this case, the buyer return is 9. A recent paper by Dogan and Yildirim (2017) actually considers how automation might impact on worker contracts. However, they do not examine AI per se, and focus on how it might change objective performance measures in teams moving from joint performance evaluation to more relative performance evaluation. You are reading copyrighted material published by University of Chicago Press. Unauthorized posting, copying, or distributing of this work except as permitted under U.S. copyright law is illegal and injures the author and publisher. **108 Ajay Agrawal, Joshua Gans, and Avi Goldfarb** ## k vR ( + (1 v ) S ) + max { [ˆ] ( m k ) vR ( + (1 v ) S ), S } + ( N m ) S p zc [( )][ˆ], while the seller return is: _p_ - (1 – _z_ ) _c_ ( [ˆ] ). Here _p_ + _zc_ ( [ˆ] ) is the contract price and _z_ is 0 for a fi xed price contract and 1 for a cost- plus contract. Note that only with a cost- plus contract does the seller exercise any judgment. Thus, the buyer chooses a cost- plus over a fi xed price contract if _k vR_ ( + (1 _v_ ) _S_ ) + max { [ˆ] ( _m_ _k_ ) _vR_ ( + (1 _v_ ) _S_ ), _S_ } [+][ (] _[N]_ _m_ ) _S_ _c_ [( )][ˆ] - _k vR_ ( + (1 _v_ ) _S_ ) + ( _N_ _k_ ) _S_ . It is easy to see that as _m_ rises (i.e., prediction becomes cheaper), a cost- plus contract is more likely to be chosen. That is, incentives fall as prediction becomes more abundant. Now we can consider the impact of integration. We assume that the buyer can choose to make the decisions themselves, but at a higher cost. That is, _c_ ( [ˆ], _I_ ) > _c_ ( [ˆ] ) where _I_ denotes integration. We also assume that ∂ _c_ ( [ˆ], _I_ )/ ∂ [ˆ] ( _c_ ( [ˆ] ) / ˆ ). Under integration, the buyer’s value is _k vR_ ( + (1 _v_ ) _S_ ) + [ˆ][ *] ( _m_ _k_ ) _vR_ ( + (1 _v_ ) _S_ ) + ( _N_ _m_ ) _S_ _c_ ( [ˆ][ *], _I_ ) where [ˆ][ *] maximizes the buyer payoff in this case. Given this, it can easily be seen that as _m_ increases, the returns to integration rise. By contrast, notice that as _k_ increases, the incentives for a cost- plus contract are diminished and the returns to integration fall. Thus, the more prediction machines allow for the placement of contingencies in a contract (the larger _m- k_ ), the higher powered will seller incentives be and the more likely there is to be integration. Forbes and Lederman (2009) showed that airlines are more likely to vertically integrate with regional partners when scheduling is more complex: specifi cally, where bad weather is more likely to lead to delays. The impact of prediction machines will depend on whether they lead to an increase in the number of states where the action can be automated in a state- contingent manner ( _k_ ) relative to the increase in the number of states where the state becomes known but the action cannot be automated ( _m_ ). If the former, then we will see more vertical integration with the rise of prediction machines. If the latter, we will see less. The diff erence is driven by the need for more costly judgment in the vertically integrated case as _m- k_ rises. **3.6 Conclusions** In this chapter, we explore the consequences of recent improvements in machine- learning technology that have advanced the broader fi eld of artifi cial intelligence. In particular, we argue that these advances in the ability of machines to conduct mental tasks are driven by improvements in machine prediction. In order to understand how improvements in machine prediction will impact decision- making, it is important to analyze how the payoff s of the model arise. We label the process of learning payoff s “judgment.” You are reading copyrighted material published by University of Chicago Press. Unauthorized posting, copying, or distributing of this work except as permitted under U.S. copyright law is illegal and injures the author and publisher. A Theory of Decision- Making and Artifi cial Intelligence **109** By modeling judgment explicitly, we derive a number of useful insights into the value of prediction. We show that prediction and judgment are generally complements, as long as judgment is not too diffi cult. We also show that improvements in judgment change the type of prediction quality that is most useful: better judgment means that more accurate predictions are valuable relative to more frequent predictions. Finally, we explore the role of complexity, demonstrating that, in the presence of complexity, the impact of improved prediction on the value of judgment depends on whether improved prediction leads to automated decision- making. Complexity is a key aspect of economic research in automation, contracting, and the boundaries of the fi rm. As prediction machines improve, our model suggests that the consequences in complex environments are particularly fruitful to study. There are numerous directions research in this area could proceed. First, the chapter does not explicitly model the form of the prediction—including what measures might be the basis for decision- making. In reality, this is an important design variable and impacts on the accuracy of predictions and decision- making. In computer science, this is referred to as the choice of surrogates, and this appears to be a topic amenable for economic theoretical investigation. Second, the chapter treats judgment as largely a human- directed activity. However, we have noted that it can else be encoded, but have not been explicit about the process by which this occurs. Endogenising this—perhaps relating it to the accumulation of experience—would be an avenue for further investigation. Finally, this is a single- agent model. It would be interesting to explore how judgment and prediction mix when each is impacted upon by the actions and decisions of other agents in a game theoretic setting. **References** Acemoglu, Daron. 2003. “Labor- and Capital- Augmenting Technical Change.” _Journal of the European Economic Association_ 1 (1): 1– 37. Acemoglu, Daron, and Pascual Restrepo. 2017. “The Race between Machine and Man: Implications of Technology for Growth, Factor Shares, and Employment.” Working paper, Massachusetts Institute of Technology. Agrawal, Ajay, Joshua S. Gans, and Avi Goldfarb. 2018a. “Human Judgment and AI Pricing.” _American Economic Association: Papers & Proceedings_, 108:58–63. ———. 2018b. _Prediction Machines: The Simple Economics of Artifi cial Intelligence._ Boston, MA: Harvard Business Review Press. Alpaydin, Ethem. 2010. _Introduction to Machine Learning_, 2nd ed. Cambridge, MA: MIT Press. Autor, David. 2015. “Why Are There Still So Many Jobs? The History and Future of Workplace Automation.” _Journal of Economic Perspectives_ 29 (3): 3– 30. Baker, George, Robert Gibbons, and Kevin Murphy. 1999. “Informal Authority in Organizations.” _Journal of Law, Economics, and Organization_ 15:56– 73. Belloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2014. “High You are reading copyrighted material published by University of Chicago Press. Unauthorized posting, copying, or distributing of this work except as permitted under U.S. copyright law is illegal and injures the author and publisher. **110 Andrea Prat** Dimensional Methods and Inference on Structural and Treatment Eff ects.” _Jour-_ _nal of Economic Perspectives_ 28 (2): 29– 50. Benzell, Seth G., Laurence J. Kotlikoff, Guillermo LaGarda, and Jeff rey D. Sachs. 2015. “Robots Are Us: Some Economics of Human Replacement.” NBER Working Paper no. 20941, Cambridge, MA. Bolton, P., and A. Faure- Grimaud. 2009. “Thinking Ahead: The Decision Problem.” _Review of Economic Studies_ 76:1205– 38. Brynjolfsson, Erik, and Andrew McAfee. 2014. _The Second Machine Age_ . New York: W. W. Norton. Dogan, M., and P. Yildirim. 2017. “Man vs. Machine: When Is Automation Inferior to Human Labor?” Unpublished manuscript, The Wharton School of the University of Pennsylvania. Domingos, Pedro. 2015. _The Master Algorithm._ New York: Basic Books. Forbes, Silke, and Mara Lederman. 2009. “Adaptation and Vertical Integration in the Airline Industry.” _American Economic Review_ 99 (5): 1831– 49. Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. _The Elements of_ _Statistical Learning: Data Mining, Inference, and Prediction_, 2nd ed. New York: Springer. Hawkins, Jeff . 2004. _On Intelligence_ . New York: Times Books. Jha, S., and E. J. Topol. 2016. “Adapting to Artifi cial Intelligence: Radiologists and Pathologists as Information Specialists.” _Journal of the American Medical Associa-_ _tion_ 316 (22): 2353– 54. Lusted, L. B. 1960. “Logical Analysis in Roentgen Diagnosis.” _Radiology_ 74:178– 93. Markov, John. 2015. _Machines of Loving Grace_ . New York: HarperCollins Publishers. Ng, Andrew. 2016. “What Artifi cial Intelligence Can and Can’t Do Right Now.” _Harvard Business Review Online_ . Accessed Dec. 8, 2016. https:// hbr .org/ 2016/ 11 / what- artifi cial- intelligence- can- and- cant- do- right- now. Simon, H. A. 1951. “A Formal Theory of the Employment Relationship.” _Econo-_ _metrica_ 19 (3): 293– 305. Tadelis, S. 2002. “Complexity, Flexibility and the Make- or- Buy Decision.” _American_ _Economic Review_ 92 (2): 433– 37. Tirole, J. 2009. “Cognition and Incomplete Contracts.” _American Economic Review_ 99 (1): 265– 94. Varian, Hal R. 2014. “Big Data: New Tricks for Econometrics.” _Journal of Economic_ _Perspectives_ 28 (2): 3– 28. **Comment** Andrea Prat One of the key activities of organizations is to collect, process, combine, and utilize information (Arrow 1974). A modern corporation exploits the vast amounts of data that it accumulates from marketing, operations, human resources, fi nance, and other functions to grow faster and be more Andrea Prat is the Richard Paul Richman Professor of Business at Columbia Business School and professor of economics at Columbia University. For acknowledgments, sources of research support, and disclosure of the author’s material fi nancial relationships, if any, please see http:// www .nber .org/ chapters/ c14022.ack. You are reading copyrighted material published by University of Chicago Press. Unauthorized posting, copying, or distributing of this work except as permitted under U.S. copyright law is illegal and injures the author and publisher.",
  "pages": null,
  "cleaned_text": "This PDF is a selection from a published volume from the National Bureau of Economic Research\n\neditors\n\nChapter Title: Prediction, Judgment, and Complexity: A Theory of Decision-Making and Artificial Intelligence\n\nChapter Author(s): Ajay Agrawal, Joshua Gans, Avi Goldfarb\n\n# **and Complexity** A Theory of Decision-Making and Artificial Intelligence\nAjay Agrawal, Joshua Gans, and Avi Goldfarb\n\n**3.1 Introduction**\nThere is widespread discussion regarding the impact of machines on employment (see Autor 2015). In some sense, the discussion mirrors a longstanding literature on the impact of the accumulation of capital equipment on employment; specifically, whether capital and labor are substitutes or complements (Acemoglu 2003). But the recent discussion is motivated by the integration of software with hardware and whether the role of machines goes beyond physical tasks to mental ones as well (Brynjolfsson and McAfee 2014). As mental tasks were seen as always being present and essential, human comparative advantage in these was seen as the main reason why, at least in the long term, capital accumulation would complement employment by enhancing labor productivity in those tasks. The computer revolution has blurred the line between physical and men\n\nAjay Agrawal is the Peter Munk Professor of Entrepreneurship at the Rotman School of Management, University of Toronto, and a research associate of the National Bureau of Economic Research. Joshua Gans is professor of strategic management and holder of the Jeffrey S. Skoll Chair of Technical Innovation and Entrepreneurship at the Rotman School of Management, University of Toronto (with a cross appointment in the Department of Economics), and a research associate of the National Bureau of Economic Research. Avi Goldfarb holds the Rotman Chair in Artificial Intelligence and Healthcare and is professor of marketing at the Rotman School of Management, University of Toronto, and is a research associate of the National Bureau of Economic Research. Our thanks to Andrea Prat, Scott Stern, Hal Varian, and participants at the AEA (Chicago), NBER Summer Institute (2017), NBER Economics of AI Conference (Toronto), Columbia Law School, Harvard Business School, MIT, and University of Toronto for helpful comments. Responsibility for all errors remains our own. The latest version of this chapter is available at joshuagans .com. For acknowledgments, sources of research support, and disclosure of the authors' material financial relationships, if any, please see http:// www .nber .org/ chapters / c14010.ack.\n\nU.S. copyright law is illegal and injures the author and publisher.\n\n**90 Ajay Agrawal, Joshua Gans, and Avi Goldfarb**\ntal tasks. For instance, the invention of the spreadsheet in the late 1970s fundamentally changed the role of bookkeepers. Prior to that invention, there was a time-intensive task involving the recomputation of outcomes in spreadsheets as data or assumptions changed. That human task was substituted by the spreadsheet software that could produce the calculations more quickly, cheaply, and frequently. However, at the same time, the spreadsheet made the jobs of accountants, analysts, and others far more productive. In the accounting books, capital was substituting for labor, but the mental productivity of labor was being changed. Thus, the impact on employment critically depended on whether there were tasks the \"computers cannot do.\" These assumptions persist in models today. Acemoglu and Restrepo (2017) observe that capital substitutes for labor in certain tasks while at the same time technological progress creates new tasks. They make what they call a \"natural assumption\" that only labor can perform the new tasks as they are more complex than previous ones. [1] Benzell et al. (2015) consider the impact of software more explicitly. Their environment has two types of labor-high-tech (who can, among other things, code) and low-tech (who are empathetic and can handle interpersonal tasks). In this environment, it is the low-tech workers who cannot be replaced by machines while the high-tech ones are employed initially to create the code that will eventually displace their kind. The results of the model depend, therefore, on a class of worker who cannot be substituted directly for capital, but also on the inability of workers themselves to substitute between classes. In this chapter, our approach is to delve into the weeds of what is happening currently in the field of artificial intelligence (AI). The recent wave of developments in AI all involve advances in machine learning. Those advances allow for automated and cheap prediction; that is, providing a forecast (or nowcast) of a variable of interest from available data (Agrawal, Gans and Goldfarb 2018b). In some cases, prediction has enabled full automation of tasks-for example, self-driving vehicles where the process of data collection, prediction of behavior and surroundings, and actions are all conducted without a human in the loop. In other cases, prediction is a standalone tool-such as image recognition or fraud detection-that may or may not lead to further substitution of human users of such tools by machines. Thus far, substitution between humans and machines has focused mainly on cost considerations. Are machines cheaper, more reliable, and more scalable (in their software form) than humans? This chapter, however, considers the role of prediction in decision-making explicitly and from that examines the complementary skills that may be matched with prediction within a task.\n\n1. To be sure, their model is designed to examine how automation of tasks causes a change in factor prices that biases innovation toward the creation of new tasks that labor is more suited to.\n\nU.S. copyright law is illegal and injures the author and publisher.\n\nA Theory of Decision-Making and Artificial Intelligence **91**\n\nOur focus, in this regard, is on what we term _judgment_ . While judgment is a term with broad meaning, here we use it to refer to a very specific skill. To see this, consider a decision. That decision involves choosing an action, _x_, from a set, _X_ . The payoff (or reward) from that action is defined by a function, _u_ ( _x_, �) where � is a realization of an uncertain state drawn from a distribution, _F_ (�). Suppose that, prior to making a decision, a _prediction_ (or signal), _s_, can be generated that results in a posterior, _F_ (�| _s_ ). Thus, the decision maker would solve\n\nIn other words, a standard problem of choice under uncertainty. In this standard world, the role of prediction is to improve decision-making. The payoff, or utility function, is known. To create a role for judgment, we depart from this standard set-up in statistical decision theory and ask how a decision maker comes to know the function, _u_ ( _x_, �)? We assume that this is not simply given or a primitive of the decision-making model. Instead, it requires a human to undertake a costly process that allows the mapping from ( _x_, �) to a particular payoffvalue, _u_, to be discovered. This is a reasonable assumption given that beyond some rudimentary experimentation in closed environments, there is no current way for an AI to impute a utility function that resides with humans. Additionally, this process separates the costs of providing the mapping for each pair, ( _x_, �). (Actually, we focus, without loss in generality, on situations where _u_ ( _x_, �) ≠ _u_ ( _x_ ) for all � and presume that if a payoffto an action is state independent that payoffis known.) In other words, while prediction can obtain a signal of the underlying state, judgment is the process by which the payoffs from actions that arise based on that state can be determined. We assume that this process of determining payoffs requires human understanding of the situation: it is not a prediction problem. For intuition on the difference between prediction and judgment, consider the example of credit card fraud. A bank observes a credit card transaction. That transaction is either legitimate or fraudulent. The decision is whether to approve the transaction. If the bank knows for sure that the transaction is legitimate, the bank will approve it. If the bank knows for sure that it is fraudulent, the bank will refuse the transaction. Why? Because the bank knows the payoffof approving a legitimate transaction is higher than the payoffof refusing that transaction. Things get more interesting if the bank is uncertain about whether the transaction is legitimate. The uncertainty means that the bank also needs to know the payofffrom refusing a legitimate transaction and from approving a fraudulent transaction. In our model, judgment is the process of determining these payoffs. It is a costly activity, in the sense that it requires time and effort. As the new developments regarding AI all involve making prediction more readily available, we ask, how does judgment and its endogenous appli\n\nU.S. copyright law is illegal and injures the author and publisher.\n\n**92 Ajay Agrawal, Joshua Gans, and Avi Goldfarb**\ncation change the value of prediction? Are prediction and judgment substitutes or complements? How does the value of prediction change monotonically with the difficulty of applying judgment? In complex environments (as they relate to automation, contracting, and the boundaries of the firm), how do improvements in prediction affect the value of judgment? We proceed by first providing supportive evidence for our assumption that recent developments in AI overwhelmingly impact the costs of prediction. We then use the example of radiology to provide a context for understanding the different roles of prediction and judgment. Drawing inspiration from Bolton and Faure-Grimaud (2009), we then build the baseline model with two states of the world and uncertainty about payoffs to actions in each state. We explore the value of judgment in the absence of any prediction technology, and then the value of prediction technology when there is no judgment. We finish the discussion of the baseline model with an exploration of the interaction between prediction and judgment, demonstrating that prediction and judgment are complements as long as judgment isn't too difficult. We then separate prediction quality into prediction frequency and prediction accuracy. As judgment improves, accuracy becomes more important relative to frequency. Finally, we examine complex environments where the number of potential states is large. Such environments are common in economic models of automation, contracting, and boundaries of the firm. We show that the effect of improvements in prediction on the importance of judgment depend a great deal on whether the improvements in prediction enable automated decision-making.\n\n**3.2 AI and Prediction Costs**\nWe argue that the recent advances in artificial intelligence are advances in the technology of prediction. Most broadly, we define prediction as the ability to take known information to generate new information. Our model emphasizes prediction about the state of the world. Most contemporary artificial intelligence research and applications come from a field now called \"machine learning.\" Many of the tools of machine learning have a long history in statistics and data analysis, and are likely familiar to economists and applied statisticians as tools for prediction and classification. [2] For example, Alpaydin's (2010) textbook _Introduction to_ _Machine Learning_ covers maximum likelihood estimation, Bayesian estimation, multivariate linear regression, principal components analysis, clustering, and nonparametric regression. In addition, it covers tools that may be less familiar, but also use independent variables to predict outcomes:\n\n2. We define prediction as known information to generate new information. Therefore, classification techniques such as clustering are prediction techniques in which the new information to be predicted is the appropriate category or class.\n\nU.S. copyright law is illegal and injures the author and publisher.\n\nA Theory of Decision-Making and Artificial Intelligence **93**\n\nregression trees, neural networks, hidden Markov models, and reinforcement learning. Hastie, Tibshirani, and Friedman (2009) cover similar topics. The 2014 _Journal of Economic Perspectives_ symposium on big data covered several of these less familiar prediction techniques in articles by Varian (2014) and Belloni, Chernozhukov, and Hansen (2014). While many of these prediction techniques are not new, recent advances in computer speed, data collection, data storage, and the prediction methods themselves have led to substantial improvements. These improvements have transformed the computer science research field of artificial intelligence. The Oxford English Dictionary defines artificial intelligence as \"[t]he theory and development of computer systems able to perform tasks normally requiring human intelligence.\" In the 1960s and 1970s, artificial intelligence research was primarily rules-based, symbolic logic. It involved human experts generating rules that an algorithm could follow (Domingos 2015, 89). These are not prediction technologies. Such systems became very good chess players and they guided factory robots in highly controlled settings; however, by the 1980s, it became clear that rules-based systems could not deal with the complexity of many nonartificial settings. This led to an \"AI winter\" in which research funding artificial intelligence projects largely dried up (Markov 2015). Over the past ten years, a different approach to artificial intelligence has taken off . The idea is to program computers to \"learn\" from example data or experience. In the absence of the ability to predetermine the decision rules, a data-driven prediction approach can conduct many mental tasks. For example, humans are good at recognizing familiar faces, but we would struggle to explain and codify this skill. By connecting data on names to image data on faces, machine learning solves this problem by predicting which image data patterns are associated with which names. As a prominent artificial intelligence researcher put it, \"Almost all of AI's recent progress is through one type, in which some input data (A) is used to quickly generate some simple response (B)\" (Ng 2016). Thus, the progress is explicitly about improvements in prediction. In other words, the suite of technologies that have given rise to the recent resurgence of interest in artificial intelligence use data collected from sensors, images, videos, typed notes, or anything else that can be represented in bits to fill in missing information, recognize objects, or forecast what will happen next. To be clear, we do not take a position on whether these prediction technologies really do mimic the core aspects of human intelligence. While Palm Computing founder JeffHawkins argues that human intelligence is-in essence-prediction (Hawkins 2004), many neuroscientists, psychologists, and others disagree. Our point is that the technologies that have been given the label artificial intelligence are prediction technologies. Therefore, in order to understand the impact of these technologies, it is important to assess the impact of prediction on decisions.\n\nU.S. copyright law is illegal and injures the author and publisher.\n\n**94 Ajay Agrawal, Joshua Gans, and Avi Goldfarb**\n**3.3 Case: Radiology**\n\nBefore proceeding to the model, we provide some intuition of how prediction and judgment apply in a particular context where prediction machines are expected to have a large impact: radiology. In 2016, GeoffHinton-one of the pioneers of deep learning neural networks-stated that it was no longer worth training radiologists. His strong implication was that radiologists would not have a future. This is something that radiologists have been concerned about since 1960 (Lusted 1960). Today, machine-learning techniques are being heavily applied in radiology by IBM using its Watson computer and by a start-up, Enlitic. Enlitic has been able to use deep learning to detect lung nodules (a fairly routine exercise) [3] but also fractures (which is more complex). Watson can now identify pulmonary embolism and some other heart issues. These advances are at the heart of Hinton's forecast, but have also been widely discussed among radiologists and pathologists (Jha and Topol 2016). What does the model in this chapter suggest about the future of radiologists? If we consider a simplified characterization of the job of a radiologist, it would be that they examine an image in order to characterize and classify that image and return an assessment to a physician. While often that assessment is a diagnosis (i.e., \"the patient has pneumonia\"), in many cases, the assessment is in the negative (i.e., \"pneumonia not excluded\"). In that regard, this is stated as a predictive task to inform the physician of the likelihood of the state of the world. Using that, the physician can devise a treatment. These predictions are what machines are aiming to provide. In particular, it might provide a differential diagnosis of the following kind:\n\n_Based on Mr Patel's demographics and imaging, the mass in the liver has a_ _66.6 percent chance of being benign, 33.3 percent chance of being malignant,_ _and a 0.1 percent of not being real._ [4]\n\nThe action is whether some intervention is needed. For instance, if a potential tumor is identified in a noninvasive scan, then this will inform whether an invasive examination will be conducted. In terms of identifying the state of the world, the invasive exam is costly but safe-it can deduce a cancer with certainty and remove it if necessary. The role of a noninvasive exam is to inform whether an invasive exam should be forgone. That is, it is to make physicians more confident about abstaining from treatment and further analysis. In this regard, if the machine improves prediction, it will lead to fewer invasive examinations.\n\n3. \"You did not go to medical school to measure lung nodules.\" http:// www .medscape .com / viewarticle/ 863127#vp_2. 4. http:// www .medscape .com/ viewarticle/ 863127#vp_3.\n\nU.S. copyright law is illegal and injures the author and publisher.\n\nA Theory of Decision-Making and Artificial Intelligence **95**\n\nJudgment involves understanding the payoffs. What is the payoffto conducting a biopsy if the mass is benign, malignant, or not real? What is the payoffto not doing anything in those three states? The issue for radiologists in particular is whether a trained specialist radiologist is in the best position to make this judgment or will it occur further along the chain of decisionmaking or involve new job classes that merge diagnostic information such as a combined radiologist/ pathologist (Jha and Topol 2016). Next, we formalize these ideas.\n\n**3.4 Baseline Model**\nOur baseline model is inspired by the \"bandit\" environment considered by Bolton and Faure-Grimaud (2009), although it departs significantly in the questions addressed and base assumptions made. Like them, in our baseline model, we suppose there are two states of the world, {�1,�2} with prior probabilities of {�,1 - �}. There are two possible actions: a state independent action with known payoffof _S_ (safe) and a state dependent action with two possible payoffs, _R_ or _r,_ as the case may be (risky). As noted in the introduction, a key departure from the usual assumptions of rational decision-making is that the decision maker does not know the payofffrom the risky action in each state and must apply _judgment_ to determine that payoff . [5] Moreover, decision makers need to be able to make a judgment for each state that might arise in order to formulate a plan that would be the equivalent of payoffmaximization. In the absence of such judgment, the ex ante expectation that the risky action is optimal in any state is _v_ (which is independent between states). To make things more concrete, we assume _R_ - _S_ - _r_ . [6] Thus, we assume that _v_ is the probability in any state that the risky payoffis _R_ rather than _r_ . This is not a conditional probability of the state. It is a statement about the payoff, given the state. In the absence of knowledge regarding the specific payoffs from the risky action, a decision can only be made on the basis of prior probabilities. Then the safe action will be chosen if\n\n$\\mu$ _vR_ ( + (1 _v_ ) _r_ ) + 1( $\\mu$) _vR_ ( + (1 _v_ ) _r_ ) = _vR_ + (1 _v_ ) _r_ _S_ .\n\n5. Bolton and Faure-Grimaud (2009) consider this step to be the equivalent of a thought experiment where thinking takes time. To the extent that our results can be interpreted as a statement about the comparative advantage of humans, we assume that only humans can do judgment. 6. Thus, we assume that the payofffunction, _u_, can only take one of three values, { _R_, _r_, _S_ }. The issue is which combinations of state realization and action lead to which payoffs. However, we assume that _S_ is the payofffrom the safe action regardless of state and so this is known to the decision maker. As it is the relative payoffs from actions that drive the results, this assumption is without loss in generality. Requiring this property of the safe action to be discovered would just add an extra cost. Implicitly, as the decision maker cannot make a decision in complete ignorance, we are assuming that the safe action's payoffcan be judged at an arbitrarily low cost.\n\nU.S. copyright law is illegal and injures the author and publisher.\n\n**96 Ajay Agrawal, Joshua Gans, and Avi Goldfarb**\nSo that the payoffis: _V_ 0 = max{ _vR_ + (1 - _v_ ) _r_, _S_ }. To make things simpler, we will focus our attention on the case where the safe action is-in the absence of prediction or judgment-the default. That is, we assume that\n\n(A1) **(Safe Default)** _vR_ + (1 - _v_ ) _r_ ≤ _S_ .\n\nThis assumption is made for simplicity only and will not change the qualitative conclusions. [7] Under (A1), in the absence of knowledge of the payofffunction or a signal of the state, the decision maker would choose _S_ .\n\n3.4.1 Judgment in the Absence of Prediction\n\nPrediction provides knowledge of the state. The process of judgment provides knowledge of the payofffunction. Judgment therefore allows the decision maker to understand which action is optimal for a given state should it arise. Suppose that this knowledge is gained without cost (as it would be assumed to do under the usual assumptions of economic rationality). In other words, the decision maker has knowledge of optimal action in a given state. Then the risky action will be chosen (a) if it is the preferred action in both states (which arises with probability _v_ [2] ); (b) if it is the preferred action in �1 but not �2 and � _R_ + (1 - �) _r_ - _S_ (with probability _v_ (1 - _v_ )); or (c) if it is the preferred action in �2 but not �1 and � _r_ + (1 - �) _R_ - _S_ (with probability _v_ (1 - _v_ )). Thus, the expected payoffis\n## v [2] R + v (1 v )max $\\mu${ R + (1 $\\mu$) r, S } + v (1 v )max $\\mu${ r + (1 $\\mu$) R, S } [+][ (1] v ) [2] S .\nNote that this is greater than _V_ 0. The reason for this is that, when there is uncertainty, judgment is valuable because it can identify actions that are dominant or dominated-that is, that might be optimal across states. In this situation, any resolution of uncertainty does not matter as it will not change the decision made. A key insight is that judgment itself can be consequential. Result 1: _If_ max{� _R_ + (1 - �) _r,_ - _r_ + (1 - �) _R_ } > _S, it is possible that_ _judgment alone can cause the decision to switch from the default action (safe)_ _to the alternative action (risky)._ As we are motivated by understanding the interplay between prediction and judgment, we want to make these consequential. Therefore, we make the following assumption to ensure prediction always has some value:\n\n(A2) **(Judgment Insufficient)** max{� _R_ + (1 - �) _r,_ - _r_ + (1 - �) _R_ } ≤ _S_ .\n\nUnder this assumption, if different actions are optimal in each state and this is known, the decision maker will not change to the risky action. This, of course, implies that the expected payoffis\n\n7. Bolton and Faure-Grimaud (2009) make the opposite assumption. Here, as our focus is on the impact of prediction, it is better to consider environments where prediction has the effect of reducing uncertainty over riskier actions.\n\nU.S. copyright law is illegal and injures the author and publisher.\n\nA Theory of Decision-Making and Artificial Intelligence **97**\n\n_v_ [2] _R_ + (1 _v_ [2] ) _S_ .\n\nNote that, absent any cost, full judgment improves the decision maker's expected payoff . Judgment does not come for free. We assume here that it takes time (although the formulation would naturally match with the notion that it takes costly effort). Suppose the discount factor is � < 1. A decision maker can spend time in a period determining what the optimal action is for a particular state. If they choose to apply judgment with respect to state � _i_, then there is a probability � _i_ that they will determine the optimal action in that period and can make a choice based on that judgment. Otherwise, they can choose to apply judgment to that problem in the next period. It is useful, at this point, to consider what judgment means once it has been applied. The initial assumption we make here is that the knowledge of the payofffunction depreciates as soon as a decision is made. In other words, applying judgment can delay a decision (and that is costly) and it can improve that decision (which is its value) but it cannot generate experience that can be applied to other decisions (including future ones). In other words, the initial conception of judgment is the application of _thought_ rather than the gathering of _experience_ . [8] Practically, this reduces our examination to a static model. However, in a later section, we consider the experience formulation and demonstrate that most of the insights of the static model carry over to the dynamic model. In summary, the timing of the game is as follows:\n\n1. At the beginning of a decision stage, the decision maker chooses whether to apply judgment and to what state or whether to simply choose an action without judgment. If an action is chosen, uncertainty is resolved and payoffs are realized and we move to a new decision stage. 2. If judgment is chosen, with probability, 1 - � _i_, they do not find out the payoffs for the risky action in that state, a period of time elapses and the game moves back to 1. With probability � _i_, the decision maker gains this knowledge. The decision maker can then take an action, uncertainty is resolved and payoffs are realized, and we move to a new decision stage (back to 1). If no action is taken, a period of time elapses and the current decision stage continues. 3. The decision maker chooses whether to apply judgment to the other state. If an action is chosen, uncertainty is resolved and payoffs are realized and we move to a new decision stage (back to 1). 4. If judgment is chosen, with probability, 1 - �- _i_, they do not find out the payoffs for the risky action in that state, a period of time elapses and the game moves back to 1. With probability �- _i_, the decision maker gains this knowledge. The decision maker then chooses an action, uncertainty\n\n8. The experience frame is considered in Agrawal, Gans, and Goldfarb (2018a).\n\nU.S. copyright law is illegal and injures the author and publisher.\n\n**98 Ajay Agrawal, Joshua Gans, and Avi Goldfarb**\n**Table 3.1** **Model parameters**\n\nParameter Description\n\n_S_ Known payofffrom the safe action _R_ Potential payofffrom the risky action in a given state _r_ Potential payofffrom the risky action in a given state $\\theta$ _i_ Label of state _i_ ∈ {1,2}\n\n- Probability of state 1\n_v_ Prior probability that the payoffin a given state is _R_ $\\lambda$ _i_ Probablilty that decision maker learns the payoffto the risky action $\\theta$ _i_ if judgment is applied for one period $\\delta$ Discount factor\n\nis resolved and payoffs are realized, and we move to a new decision stage (back to 1).\n\nWhen prediction is available, it will become available prior to the beginning of a decision stage. The various parameters are listed in table 3.1. Suppose that the decision maker focuses on judging the optimal action (i.e., assessing the payoff ) for � _i_ . Then the expected present discount payofffrom applying judgment is\n\n_i_ ( _[vR]_ [ +][ (1] _v_ ) _S_ )\n\n_t_ =2\n\n( _vR_ + (1 _v_ ) _S_ ).\n\nThe decision maker eventually can learn what to do and will earn a higher payoffthan without judgment, but will trade this offagainst a delay in the payoff . This calculation presumes that the decision maker knows the state-that\n\n- _i_ is true-prior to engaging in judgment. If this is not the case, then the\nexpected present discounted payoffto judgment on, say, �1 alone is\n\n1\n\n## (max v { $\\mu$( R + ( 1 $\\mu$) vR ( + ( 1 v ) r )) + ( 1 v ) $\\mu$( r + ( 1 $\\mu$) vR ( + ( 1 v ) r )), S })\n## (max v { $\\mu$( R + (1 $\\mu$) vR ( + (1 v ) r )), S } + (1 v ) S ),\n\nwhere the last step follows from equation (A1). To make exposition simpler, we suppose that �1 =�2 = �. In addition, let [ˆ] = / 1- (1-( ) ); [ˆ] can be given a similar interpretation to �, the quality of judgment. If the strategy were to apply judgment on one state only and then make a decision, this would be the relevant payoffto consider. However, because judgment is possible in both states, there are several cases to consider. First, the decision maker might apply judgment to both states in sequence. In this case, the expected present discounted payoffis\n\nU.S. copyright law is illegal and injures the author and publisher.\n\nA Theory of Decision-Making and Artificial Intelligence **99**\n\n## ˆ [2] ( v [2] R + v (1 v )max $\\mu${ R + (1 $\\mu$) r, S } + v (1 v )max $\\mu${ r + (1 $\\mu$) R, S } + (1 v ) [2] S )\n= [ˆ][ 2] ( _v_ [2] _R_ + (1 _v_ [2] ) _S_ ),\n\nwhere the last step follows from equation (A1). Second, the decision maker might apply judgment to, say, �1 first and then, contingent on the outcome there, apply judgment to �2. If the decision maker chooses to pursue judgment on �2 if the outcome for �1 is that the risky action is optimal, the payoffbecomes\n\nˆ( _v_ ˆ _vR_ ( + (1 _v_ )max $\\mu${ _R_ + (1 $\\mu$) _r_, _S_ })\n\n+ (1 _v_ )max $\\mu${ _r_ + (1 $\\mu$) _vR_ ( + (1 _v_ ) _r_ ), _S_ })\n\n= [ˆ] ( _v_ [ˆ] ( _vR_ + (1 _v_ ) _S_ ) + (1 _v_ ) _S_ ).\n\nIf the decision maker chooses to pursue judgment on �2 after determining that the outcome for �1 is that the safe action is optimal, the payoffbecomes\n\nˆ( _v_ max $\\mu${ _R_ + (1 $\\mu$) _vR_ ( + (1 _v_ ) _r_ ), _S_ }\n\n+ (1 _v_ ) [ˆ] ( _v_ max $\\mu${ _r_ + (1 $\\mu$) _R_, _S_ } + (1 _v_ ) _S_ ))\n## = [ˆ] ( v max $\\mu${ R + (1 $\\mu$) vR ( + (1 v ) r ), S } + (1 v ) [ˆ] S ).\nNote that this is option is dominated by not applying further judgment at all if the outcome for �1 is that the safe action is optimal. Given this we can prove the following:\n\nProposition 1: _Under (A1) and (A2), and in the absence of any signal_ _about the state, (a) judging both states and (b) continuing after the discovery_ _that the safe action is preferred in a state are never optimal._\n\nProof: Note that judging two states is optimal if\n\n_S_ ˆ > _v_ max $\\mu${ _r_ + (1 $\\mu$) _R_, _S_ } [+][ (1] _v_ ) _S_\n\n$\\mu$ _R_ + (1 $\\mu$) _vR_ ( + (1 _v_ ) _r_ ) ˆ > ~~.~~\n\n_vR_ + (1 _v_ )max $\\mu${ _R_ + (1 $\\mu$) _r_, _S_ }\n\nAs (A2) implies that � _r_ + (1 - �) _R_ ≤ _S_, the first condition reduces to ˆ\n- 1. Thus, (a) judging two states is dominated by judging one state and\ncontinuing to explore only if the risk is found to be optimal in that state. Turning to the strategy of continuing to apply judgment only if the safe action is found to be preferred in a state, we can compare this to the payofffrom applying judgment to one state and then acting immediately. Note that\n\nU.S. copyright law is illegal and injures the author and publisher.\n\n**100 Ajay Agrawal, Joshua Gans, and Avi Goldfarb**\n## ˆ v ( max $\\mu${ R + (1 $\\mu$) vR ( + (1 v ) r ), S } + (1 v ) [ˆ] S ) > [ˆ] ( v max $\\mu${ R + (1 $\\mu$) vR ( + (1 v ) r ), S } + (1 v ) S ) [.]\nThis can never hold, proving that (b) is dominated.\n\nThe intuition is similar to Propositions 1 and 2 in Bolton and FaureGrimaud (2009). In particular, applying judgment is only useful if it is going to lead to the decision maker switching to the risky action. Thus, it is never worthwhile to unconditionally explore a second state as it may not change the action taken. Similarly, if judging one state leads to knowledge the safe action continues to be optimal in that state, in the presence of uncertainty about the state, even if knowledge is gained of the payoffto the risky action in the second state, that action will never be chosen. Hence, further judgment is not worthwhile. Hence, it is better to choose immediately at that point rather than delay the inevitable. Given this proposition, there are only two strategies that are potentially optimal (in the absence of prediction). One strategy (we will term here J1) is where judgment is applied to one state and if the risky action is optimal, then that action is taken immediately; otherwise, the safe default is taken immediately. The state where judgment is applied first is the state most likely to arise. This will be state 1 if � > 1/ 2. This strategy might be chosen if\n## ˆ v ( max $\\mu${ R + (1 $\\mu$) vR ( + (1 v ) r ), S } + (1 v ) S ) > S\nˆ > ˆ\n\n_J_ 1\n\n_S_ _v_ max $\\mu${ _R_ + (1 $\\mu$) _vR_ ( + (1 _v_ ) _r_ ), _S_ } [+][ (1] _v_ ) _S_\n\n,\n\nwhich clearly requires that � _R_ + (1 - �)( _vR_ + (1 - _v_ ) _r_ ) > S. The other strategy (we will term here J2) is where judgment is applied to one state and if the risky action is optimal, then judgment is applied to the next state; otherwise, the safe default is taken immediately. Note that J2 is preferred to J1 if\n\nˆ _v_ ( ˆ _vR_ ( + (1 _v_ ) _S_ ) + (1 _v_ ) _S_ )\n## > [ˆ] ( v max $\\mu${ R + (1 $\\mu$) vR ( + (1 v ) r ), S } + (1 v ) S )\nˆ _v vR_ ( + (1 _v_ ) _S_ ) > _v_ max $\\mu${ _R_ + (1 $\\mu$) _vR_ ( + (1 _v_ ) _r_ ), _S_ }\n\nmax $\\mu${ _R_ + (1 $\\mu$) _vR_ ( + (1 _v_ ) _r_ ), _S_ } ˆ > ~~.~~\n\n_vR_ + (1 _v_ ) _S_\n\nThis is intuitive. Basically, it is only when the efficiency of judgment is sufficiently high that more judgment is applied. However, for this inequality to be relevant, J2 must also be preferred to the status quo yielding a payoffof _S_ . Thus, J2 is not dominated if\n\nU.S. copyright law is illegal and injures the author and publisher.\n\nA Theory of Decision-Making and Artificial Intelligence **101**\n\n,\n\n,\n\nwhere the first term is the range where J2 dominates J1, while the second term is where J2 dominates _S_ alone; so for J2 to be optimal, it must exceed both. Note also that as � → ( _S_ - _r_ )/ ( _R_ - _r_ ) (its highest possible level consistent with [A1] and [A2]), then [ˆ] _J_ 2 [→] [ 1.]\n\nIf � _R_ + (1 - �)( _vR_ + (1 - _v_ ) _r_ ) > S, note that\n\n_S_\n\n_v_ $\\mu$( _R_ + (1 $\\mu$)( _vR_ + (1 _v_ ) _r_ ))+ (1 _v_ ) _S_\n\nˆ\n\n_J_ 2 [>][ ˆ]\n\n_J_ 1\n\n$\\mu$ _R_ + (1 $\\mu$) _vR_ ( + (1 _v_ ) _r_ )\n\n_vR_ + (1 _v_ ) _S_\n\n## (1 v ) S $\\mu$( R + (1 $\\mu$) vR ( + (1 v ) r ) S )> v RS ( ($\\mu$ R + (1 $\\mu$)( vR + (1 v ) r ))2) [,]\nwhich may not hold for _v_ sufficiently high. However, it can be shown that when [ˆ] _J_ 2 [ + ] [ˆ] _J_ 1 [, then the two terms of ] [ˆ] _J_ 2 [ are equal and the second term ]\n\nexceeds the first when [ˆ] _J_ 2 [ˆ] _J_ 1 [. This implies that in the range where ][ˆ] _J_ 2 [ < ][ˆ] _J_ 1 [, ]\n\nJ2 dominates J1. This analysis implies there are two types of regimes with judgment only. If [ˆ] [ > ] [ˆ] [, then easier decisions (with high ] [ˆ][) involve using J2, the next ]\n\n_J_ 2 [ + ] [ˆ]\n\n_J_ 1 [, then the two terms of ] [ˆ]\n\n_J_ 2 [ˆ]\n\n_J_ 2 [ < ][ˆ]\n\n_J_ 1 [. This implies that in the range where ][ˆ]\n\nIf [ˆ] _J_ 2 [ > ] [ˆ] _J_ 1 [, then easier decisions (with high ] [ˆ][) involve using J2, the next ]\n\ntranche of decisions use J1 (with intermediate [ˆ] ) while the remainder in volves no exercise of judgment at all. On the other hand, if [ˆ] _J_ 2 [ < ][ˆ] _J_ 1 [, then ]\n\nthe easier decisions involve using J2 while the remainder do not involve judgment at all.\n\n_J_ 2 [ > ] [ˆ]\n\n_J_ 2 [ < ][ˆ]\n\n3.4.2 Prediction in the Absence of Judgment\n\nNext, we consider the model with prediction but no judgment. Suppose that there exists an AI that can, if deployed, identify the state prior to a decision being made. In other words, prediction, if it occurs, is perfect; an assumption we will relax in a later section. Initially, suppose there is no judgment mechanism to determine what the optimal action is in each state. Recall that, in the absence of prediction or judgment, (A1) ensures that the safe action will be chosen. If the decision maker knows the state, then the risky action in a given state is chosen if\n\n_vR_ + (1 - _v_ ) _r_ - S.\n\nThis contradicts (A1). Thus, the expected payoffis\n\n_VP_ = _S_,\n\nwhich is the same outcome if there is no judgment or prediction.\n\n3.4.3 Prediction and Judgment Together\n\nBoth prediction and judgment can be valuable on their own. The question we next wish to consider is whether they are complements or substitutes. While perfect prediction allows you to choose an action based on the\n\nU.S. copyright law is illegal and injures the author and publisher.\n\n**102 Ajay Agrawal, Joshua Gans, and Avi Goldfarb**\nactual rather than expected state, it also affords the same opportunity with respect to judgment. As judgment is costly, it is useful not to waste considering what action might be taken in a state that does not arise. This was not possible when there was no prediction. But if you receive a prediction regarding the state, you can then apply judgment exclusively to actions in relation to that state. To be sure, that judgment still involves a cost, but at the same time does not lead to any wasted cognitive resources. Given this, if the decision maker were the apply judgment after the state is predicted, their expected discounted payoffwould be\n\n_VPJ_ = max { [ˆ] ( _vR_ + (1 _v_ ) _S_ ), _S_ }.\n\nThis represents the highest expected payoffpossible (net of the costs of judgment). A necessary condition for both prediction and judgment to be optimal is that: [ˆ] ≥ [ˆ] [≡] _[s]_ [/ [] _[vR]_ [ + (1 - ] _[v]_ [)] _[S]_ []. Note that ][ˆ] [ ≤ ][ˆ] [, ][ˆ] [.]\n\n_PJ_ [≡] _[s]_ [/ [] _[vR]_ [ + (1 - ] _[v]_ [)] _[S]_ []. Note that ][ˆ]\n\n_PJ_ [ ≤ ][ˆ]\n\n_J_ 1 [, ][ˆ]\n\n_J_ 2 [.]\n\n3.4.4 Complements or Substitutes?\n\nTo evaluate whether prediction and judgment are complements or substitutes, we adopt the following parameterization for the effectiveness of prediction: we assume that with probability _e_ an AI yields a prediction, while otherwise, the decision must be made in its absence (with judgment only). With this parameterization, we can prove the following:\n\nProposition 2: _In the range of_ - _where_ [ˆ] _<_ [ˆ] _J2_ _[, e and ]_ [�] _[ are complements, ]_\n\n_otherwise they are substitutes._\n\nProof: Step 1. Is [ˆ] _J_ 2 [ > ] _[R]_ [/ [2(] _[vR]_ [ + (1 - ] _[v]_ [)] _[S]_ [)]? First, note that]\n\nmax $\\mu${ _R_ + (1 $\\mu$)( _vR_ + (1 _v_ ) _r_ ), _S_ } _R_\n\n_vR_ + (1 _v_ ) _S_ 2 _vR_ ( + (1 _v_ ) _S_ )\n\n## max $\\mu${ R + (1 $\\mu$) vR ( + (1 v ) r ) [,] [S] } > [1]\n2 _[R]_ [.]\n\nNote that by (A2) and since � > (1/ 2), _S_ - � _R_ + (1 - �) _r_ - (1/ 2) _R_ so this inequality always holds. Second, note that\n\n2 _v vR_ ( + (1 _v_ ) _S_ ) 2 _vR_ ( + (1 _v_ ) _S_ )\n\n_S_ 4( _v_ [2] _R_ + _S_ (1 + 2 _v_ 3 _v_ [2] )) > _vR_ ( + (1 _v_ ) _S_ )2\n\n_S_ ( _S_ 2 _R_ ) > _v_ ( _R_ [2] 6 _RS_ + _S_ [2] ),\n\nwhich holds as the left-hand side is always positive while the right-hand side is always negative. Step 2: Suppose that �R + (1 - �)( _vR_ + (1 - _v_ ) _r_ ) ≤ _S_ ; then J1 is never optimal. In this case, the expected payoffis\n\nU.S. copyright law is illegal and injures the author and publisher.\n\nA Theory of Decision-Making and Artificial Intelligence **103**\n\n_eVPJ_ + (1 _e_ ) _VJ_ 2 = _e_ [ˆ] ( _vR_ + (1 _v_ ) _S_ ) + (1 _e_ ) [ˆ] ( _v_ [ˆ] ( _vR_ + (1 _v_ ) _S_ ) + (1 _v_ ) _S_ ) [.]\n\nThis mixed partial derivative with respect to (e, [ˆ] ) is _v_ ( _R_ - 2 [ˆ] ( _vR_ + (1 - _v_ ) _S_ )). This is positive if _R_ / [2( _vR_ + (1 - _v_ ) _S_ )] ≥ [ˆ] . By Step 1, this implies that for [ˆ] < [ˆ] _J_ 2 [, prediction and judgment are complements; otherwise, they ]\n\nare substitutes.\n\nStep 3: Suppose that that � _R_ + (1 - �)( _vR_ + (1 - _v_ ) _r_ ) > _S_ . Note that for ˆ _J_ 1 ˆ < ˆ _J_ 2, J1 is preferred to J2. In this case, the expected payoffto prediction and judgment is\n\n_e_ [ˆ] ( _vR_ + (1 _v_ ) _S_ )+ (1 _e_ ) [ˆ] ( _v_ max{ $\\mu$ _R_ + (1 $\\mu$) _vR_ ( + (1 _v_ ) _r_ ), _S_ } + (1 _v_ ) _S_ ).\n\nThis mixed partial derivative with respect to (e, [ˆ] ) is _v_ ( _R_ - max{� _R_ + (1 - �)( _vR_ + (1 - _v_ ) _r_ ), _S_ }) > 0. By Step 1, this implies that for [ˆ] < [ˆ] _J_ 2 [, predic-]\n\ntion and judgment are complements; otherwise, they are substitutes.\n\nThe intuition is as follows. When [ˆ] < [ˆ] _J_ 2 [, then, in the absence of prediction ]\n\neither no judgment is applied or, alternatively, strategy J1 (with one round of judgment) is optimal; _e_ parameterizes the degree of difference between the expected value with both prediction and judgment and the expected value without prediction with an increase in �, increasing both. However, with one round of judgment, the increase when judgment is used alone is less than that when both are used together. Thus, when [ˆ] < [ˆ] _J_ 2 [, prediction ]\n\nand judgment are complements. By contrast, when [ˆ] - [ˆ] _J_ 2 [, then strategy J2 (with two rounds of judgment) ]\n\nis used in the absence of prediction. In this case, increasing � increases the expected payofffrom judgment alone disproportionately more because judgment is applied on both states, whereas under prediction and judgment it is only applied on one. Thus, improving the quality of judgment reduces the returns to prediction. And so, when [ˆ] - [ˆ] _J_ 2 [, prediction and judgment are ]\n\nsubstitutes.\n\n**3.5 Complexity**\nThus far, the model illustrates the interplay between knowing the reward function (judgment) and prediction. While those results show that prediction and judgment can be substitutes, there is a sense in which they are more naturally complements. The reason is this: what prediction enables is a form of state-contingent decision-making. Without a prediction, a decision maker is forced to make the same choice regardless of the state that might arise. In the spirit of Herbert Simon, one might call this a heuristic. And in the absence of prediction, the role of judgment is to make that choice. Moreover, that choice is easier-that is, more likely to be optimal-when there exists dominant (or \"near dominant\") choices. Thus, when either the state space or the action space expand (as it may in more complex situations), it is\n\nU.S. copyright law is illegal and injures the author and publisher.\n\n**104 Ajay Agrawal, Joshua Gans, and Avi Goldfarb**\nless likely that there will exist a dominant choice. In that regard, faced with complexity, in the absence of prediction, the value of judgment diminishes and we are more likely to see decision makers choose default actions that, on average, are likely to be better than others. Suppose now we add a prediction machine to the mix. While in our model such a machine, when it renders a prediction, can perfectly signal the state that will arise, let us consider a more convenient alternative that may arise in complex situations: the prediction machine can perfectly signal some states (should they arise), but for other states no precise prediction is possible except for the fact that one of those states is the correct one. In other words, the prediction machine can sometimes render a fine prediction and otherwise a coarse one. Here, an improvement in the prediction machine means an increase in the number of states in which the machine can render a fine prediction. Thus, consider an _N_ - state model where the probability of state _i_ is � _i_ . Suppose that states {1, . . ., _m_ } can be finely predicted by an AI, while the remainder cannot be distinguished. Suppose that in the states that cannot be distinguished applying judgment is not worthwhile so that the optimal choice is the safe action. Also, assume that when a prediction is available, judgment is worthwhile; that is, [ˆ] ≥ _s_ / [ _vR_ + (1 - _v_ ) _S_ ]. In this situation, the expected present discounted value when both prediction and judgment are available is\n\n_m_\n\n_i_ =1\n\n$\\mu$ _i vR_ ( + (1 _v_ ) _S_ ) +\n\n_N_\n\n$\\mu$ _iS_ .\n\n_i_ = _m_ +1\n\nSimilarly, it is easy to see that _VP_ = _VJ_ = _S_ = _V_ 0 as _vR_ + (1 - _v_ ) _r_ ≤ _S_ . Note that as _m_ increases (perhaps because the prediction machine learns to predict more states), then the marginal value of better judgment increases. That is,\n\nˆ $\\mu$ _m vR_ ( + (1 _v_ ) _S_ ) $\\mu$ _mS_ is increasing in [ˆ] . What happens as the situation becomes more complex (that is, _N_ increases)? An increase in _N_ will weakly lead to a reduction in � _i_ for any given _i_ . Holding _m_ fixed (and so the quality of the prediction machine does not improve with the complexity of the world), this will reduce the value of prediction and judgment as greater weight is placed on states where prediction is unavailable; that is, it is assumed that the increase in complexity does not, ceteris paribus, create a state where prediction is available. Thus, complexity appears to be associated with _lower_ returns to both prediction and judgment. Put differently, an improvement in prediction machines would mean _m_ increases with _N_ fixed. In this case, the returns to judgment rise as greater weight is put on states where prediction is available. This insight is useful because there are several places in the economics literature where complexity has interacted with other economic decisions. These include automation, contracting, and firm boundaries. We discuss each of these in turn, highlighting potential implications.\n\nU.S. copyright law is illegal and injures the author and publisher.\n\nA Theory of Decision-Making and Artificial Intelligence **105**\n\n3.5.1 Automation\n\nThe literature on automation is sometimes synonymous with AI. This arises because AI may power new robots that are able to operate in open environments thanks to machine learning. For instance, while automated trains have been possible for some time since they run on tracks, automated cars are new because they need to operate in far more complex environments. It is prediction in those open environments that has allowed the emergence of environmentally flexible capital equipment. Note that leads to the implication that as AI improves, tasks in more complex environments can be handled by machines (Acemoglu and Restrepo 2017). However, this story masks the message that emerges from our analysis that recent AI developments are all about prediction. Why prediction enables automated vehicles is because it is relatively straightforward to describe (and hence, program) what those vehicles should do in different situations. In other words, if prediction enables \"state contingent decisions,\" then automated vehicles arise because someone knows what decision is optimal in each state. In other words, automation means that judgment can be encoded in machine behavior. Prediction added to that means that automated capital can be moved into more complex environments. In that respect, it is perhaps natural to suggest that improvements in AI will lead to a substitution of humans for machines as more tasks in more complex environments become capable of being programmed in a state-contingent manner. That said, there is another dimension of substitution that arises in complex environments. As noted above, when states cannot be predicted (something that for a given technology is more likely to be the case in more complex environments), then the actions chosen are more likely to be defaults or the results of heuristics that perform, on average, well. Many, including Acemoglu and Restrepo (2017), argue that it is for more complex tasks that humans have a comparative advantage relative to machines. However, this is not at all obvious. If it is known that a particular default or heuristic should be used, then a machine can be programmed to undertake this. In this regard, the most complex tasks-precisely because little is known regarding how to take better actions given that the prediction of the state is coarse-may be more, not less, amenable to automation. If we had to speculate, imagine that states were ordered in terms of diminished likelihood (i.e., � _i_ ≥ � _j_ for all _i_ < _j_ ). The lowest index states might be ones that, because they arrive frequently, there is knowledge of what the optimal action is in each and so they can be programmed to be handled by a machine. The highest index states similarly, because the optimal action that cannot be determined can also be programmed. It is the intermediate states that arise less frequently but not infrequently where, if a reliable prediction existed, could be handled by humans applying judgment when those states arose. Thus, the payoffcould be written\n\nU.S. copyright law is illegal and injures the author and publisher.\n\n**106 Ajay Agrawal, Joshua Gans, and Avi Goldfarb**\n_N_\n\n$\\mu$ _iS_,\n\n_i_ = _m_ +1\n\n_m_\n\n$\\mu$ _i vR_ ( + (1 _v_ ) _S_ ) +\n\n_i_ = _k_ +1\n\n_VPJ_ =\n\n_k_\n\n_i_ =1\n\nwhere tasks 1 through _k_ are automated using prediction because there is knowledge of the optimal action. If this was the matching of tasks to machines and humans, then it is not at all clear whether an increase in complexity would be associated with more or less human employment. That said, the issue for the automation literature is not subtleties over the term \"complex tasks,\" but as AI becomes more prevalent, where might the substitution of machines for humans arise. As noted above, an increase in AI increases _m_ . At this margin, humans are able to come into the marginal tasks and, because a prediction machine is available, use judgment to conduct state-contingent decisions in those situations. Absent other effects, therefore, an increase in AI is associated with more human labor on any given task. However, as the weight on those marginal tasks is falling in the level of complexity, it may not be the more complex tasks that humans are performing more of. On the other hand, one can imagine that in a model with a full labor market equilibrium that an increase in AI that enables more human judgment at the margin may also create opportunities to study that judgment to see if it can be programmed into lower index states and be handled by machines. So, while the AI does not necessarily cause more routine tasks to be handled by machines, it might create the economic conditions that lead to just that.\n\n3.5.2 Contracting\n\nContracting shares much with programming. Here is Jean Tirole (2009, 265) on the subject:\n\nIts general thrust goes as follows. The parties to a contract (buyer, seller) initially avail themselves of an available design, perhaps an industry standard. This design or contract is the best contract under existing knowledge. The parties are unaware, however, of the contract's implications, but they realize that something may go wrong with this contract; indeed, they may exert cognitive effort in order to find out about what may go wrong and how to draft the contract accordingly: put differently, a _contingency_ is foreseeable (perhaps at a prohibitively high cost), but not necessarily foreseen. To take a trivial example, the possibility that the price of oil increases, implying that the contract should be indexed on it, is perfectly foreseeable, but this does not imply that parties will think about this possibility and index the contract price accordingly.\n\nTirole argues that contingencies can be planned for in contracts using cognitive effort (akin to what we have termed here as judgment), while others may be optimally left out because the effort is too costly relative to the return given, say, the low likelihood that contingency arises. This logic can assist us in understanding what prediction machines might\n\nU.S. copyright law is illegal and injures the author and publisher.\n\nA Theory of Decision-Making and Artificial Intelligence **107**\n\ndo to contracts. If an AI becomes available then, in writing contracts, it is possible, because fine state predictions are possible, to incur cognitive costs to determine what the contingencies should be if those states should arise. For other states, the contract will be left incomplete-perhaps for a default action or alternatively some renegotiation process. A direct implication of this is that contracts may well become less incomplete. Of course, when it comes to employment contracts, the effects may be different. As Herbert Simon (1951) noted, employment contracts differ from other contracts precisely because it is often not possible to specify what actions should be performed in what circumstance. Hence, what those contracts often allocate are different decision rights. What is of interest here is the notion that contacts can be specified clearly-that is, programmed-but also that prediction can activate the use of human judgment. That latter notion means that actions cannot be easily contracted-by definition, contractibility is programming and needing judgment implies that programming was not possible. Thus, as prediction machines improve and more human judgment is optimal, then that judgment will be applied outside of objective contract measures-including objective performance measures. If we had to speculate, this would favor more subjective performance processes, including relational contracts (Baker, Gibbons, and Murphy 1999). [9]\n\n3.5.3 Firm Boundaries\n\nWe now turn to consider what impact AI may have on firm boundaries (that is, the make or buy decision). Suppose that it is a buyer ( _B_ ) who receives the value from a decision taken-that is, the payofffrom the risky or safe action as the case may be. To make things simple, let's assume that � _i_ = � for all _i_, so that _V_ = _k vR_ ( + (1 _v_ ) _S_ ) + [ˆ] ( _m_ _k_ ) _vR_ ( + (1 _v_ ) _S_ ) + ( _N_ _m_ ) _S_ . We suppose that the tasks are undertaken by a seller ( _S_ ). The tasks {1, . . ., _k_ } and { _m_ + 1, . . ., _N_ ) can be contracted upon, while the intermediate tasks require the seller to exercise judgment. We suppose that the cost of providing judgment is a function _c_ ( [ˆ] ), which is nondecreasing and convex. (We write this function in terms of [ˆ] just to keep the notation simple.) The costs can be anticipated by the buyer. So if one of the intermediate states arises, the buyer can choose to give the seller a fixed price contract (and bear none of the costs) or a cost-plus contract (and bear all of them). Following Tadelis (2002), we assume that the seller market is competitive and so all surplus accrues to the buyer. In this case, the buyer return is\n\n9. A recent paper by Dogan and Yildirim (2017) actually considers how automation might impact on worker contracts. However, they do not examine AI per se, and focus on how it might change objective performance measures in teams moving from joint performance evaluation to more relative performance evaluation.\n\nU.S. copyright law is illegal and injures the author and publisher.\n\n**108 Ajay Agrawal, Joshua Gans, and Avi Goldfarb**\n## k vR ( + (1 v ) S ) + max { [ˆ] ( m k ) vR ( + (1 v ) S ), S } + ( N m ) S p zc [( )][ˆ],\nwhile the seller return is: _p_ - (1 - _z_ ) _c_ ( [ˆ] ). Here _p_ + _zc_ ( [ˆ] ) is the contract price and _z_ is 0 for a fixed price contract and 1 for a cost-plus contract. Note that only with a cost-plus contract does the seller exercise any judgment. Thus, the buyer chooses a cost-plus over a fixed price contract if\n\n_k vR_ ( + (1 _v_ ) _S_ ) + max { [ˆ] ( _m_ _k_ ) _vR_ ( + (1 _v_ ) _S_ ), _S_ } [+][ (] _[N]_ _m_ ) _S_ _c_ [( )][ˆ]\n\n- _k vR_ ( + (1 _v_ ) _S_ ) + ( _N_ _k_ ) _S_ .\n\nIt is easy to see that as _m_ rises (i.e., prediction becomes cheaper), a cost-plus contract is more likely to be chosen. That is, incentives fall as prediction becomes more abundant. Now we can consider the impact of integration. We assume that the buyer can choose to make the decisions themselves, but at a higher cost. That is, _c_ ( [ˆ], _I_ ) > _c_ ( [ˆ] ) where _I_ denotes integration. We also assume that ∂ _c_ ( [ˆ], _I_ )/ ∂ [ˆ] ( _c_ ( [ˆ] ) / ˆ ). Under integration, the buyer's value is\n\n_k vR_ ( + (1 _v_ ) _S_ ) + [ˆ][ *] ( _m_ _k_ ) _vR_ ( + (1 _v_ ) _S_ ) + ( _N_ _m_ ) _S_ _c_ ( [ˆ][ *], _I_ )\n\nwhere [ˆ][ *] maximizes the buyer payoffin this case. Given this, it can easily be seen that as _m_ increases, the returns to integration rise. By contrast, notice that as _k_ increases, the incentives for a cost-plus contract are diminished and the returns to integration fall. Thus, the more prediction machines allow for the placement of contingencies in a contract (the larger _m-k_ ), the higher powered will seller incentives be and the more likely there is to be integration. Forbes and Lederman (2009) showed that airlines are more likely to vertically integrate with regional partners when scheduling is more complex: specifically, where bad weather is more likely to lead to delays. The impact of prediction machines will depend on whether they lead to an increase in the number of states where the action can be automated in a state-contingent manner ( _k_ ) relative to the increase in the number of states where the state becomes known but the action cannot be automated ( _m_ ). If the former, then we will see more vertical integration with the rise of prediction machines. If the latter, we will see less. The difference is driven by the need for more costly judgment in the vertically integrated case as _m-k_ rises.\n\n**3.6 Conclusions**\nIn this chapter, we explore the consequences of recent improvements in machine-learning technology that have advanced the broader field of artificial intelligence. In particular, we argue that these advances in the ability of machines to conduct mental tasks are driven by improvements in machine prediction. In order to understand how improvements in machine prediction will impact decision-making, it is important to analyze how the payoffs of the model arise. We label the process of learning payoffs \"judgment.\"\n\nU.S. copyright law is illegal and injures the author and publisher.\n\nA Theory of Decision-Making and Artificial Intelligence **109**\n\nBy modeling judgment explicitly, we derive a number of useful insights into the value of prediction. We show that prediction and judgment are generally complements, as long as judgment is not too difficult. We also show that improvements in judgment change the type of prediction quality that is most useful: better judgment means that more accurate predictions are valuable relative to more frequent predictions. Finally, we explore the role of complexity, demonstrating that, in the presence of complexity, the impact of improved prediction on the value of judgment depends on whether improved prediction leads to automated decision-making. Complexity is a key aspect of economic research in automation, contracting, and the boundaries of the firm. As prediction machines improve, our model suggests that the consequences in complex environments are particularly fruitful to study. There are numerous directions research in this area could proceed. First, the chapter does not explicitly model the form of the prediction-including what measures might be the basis for decision-making. In reality, this is an important design variable and impacts on the accuracy of predictions and decision-making. In computer science, this is referred to as the choice of surrogates, and this appears to be a topic amenable for economic theoretical investigation. Second, the chapter treats judgment as largely a human-directed activity. However, we have noted that it can else be encoded, but have not been explicit about the process by which this occurs. Endogenising this-perhaps relating it to the accumulation of experience-would be an avenue for further investigation. Finally, this is a single-agent model. It would be interesting to explore how judgment and prediction mix when each is impacted upon by the actions and decisions of other agents in a game theoretic setting.\n\n**",
  "references_text": "References**\nAcemoglu, Daron. 2003. \"Labor-and Capital-Augmenting Technical Change.\" _Journal of the European Economic Association_ 1 (1): 1- 37. Acemoglu, Daron, and Pascual Restrepo. 2017. \"The Race between Machine and Man: Implications of Technology for Growth, Factor Shares, and Employment.\" Working paper, Massachusetts Institute of Technology. Agrawal, Ajay, Joshua S. Gans, and Avi Goldfarb. 2018a. \"Human Judgment and AI Pricing.\" _American Economic Association: Papers & Proceedings_, 108:58-63. ---. 2018b. _Prediction Machines: The Simple Economics of Artificial Intelligence._ Boston, MA: Harvard Business Review Press. Alpaydin, Ethem. 2010. _Introduction to Machine Learning_, 2nd ed. Cambridge, MA: MIT Press. Autor, David. 2015. \"Why Are There Still So Many Jobs? The History and Future of Workplace Automation.\" _Journal of Economic Perspectives_ 29 (3): 3- 30. Baker, George, Robert Gibbons, and Kevin Murphy. 1999. \"Informal Authority in Organizations.\" _Journal of Law, Economics, and Organization_ 15:56- 73. Belloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2014. \"High\n\nU.S. copyright law is illegal and injures the author and publisher.\n\n**110 Andrea Prat**\nDimensional Methods and Inference on Structural and Treatment Effects.\" _Jour-_ _nal of Economic Perspectives_ 28 (2): 29- 50. Benzell, Seth G., Laurence J. Kotlikoff, Guillermo LaGarda, and Jeffrey D. Sachs. 2015. \"Robots Are Us: Some Economics of Human Replacement.\" NBER Working Paper no. 20941, Cambridge, MA. Bolton, P., and A. Faure-Grimaud. 2009. \"Thinking Ahead: The Decision Problem.\" _Review of Economic Studies_ 76:1205- 38. Brynjolfsson, Erik, and Andrew McAfee. 2014. _The Second Machine Age_ . New York: W. W. Norton. Dogan, M., and P. Yildirim. 2017. \"Man vs. Machine: When Is Automation Inferior to Human Labor?\" Unpublished manuscript, The Wharton School of the University of Pennsylvania. Domingos, Pedro. 2015. _The Master Algorithm._ New York: Basic Books. Forbes, Silke, and Mara Lederman. 2009. \"Adaptation and Vertical Integration in the Airline Industry.\" _American Economic Review_ 99 (5): 1831- 49. Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. _The Elements of_ _Statistical Learning: Data Mining, Inference, and Prediction_, 2nd ed. New York: Springer. Hawkins, Jeff . 2004. _On Intelligence_ . New York: Times Books. Jha, S., and E. J. Topol. 2016. \"Adapting to Artificial Intelligence: Radiologists and Pathologists as Information Specialists.\" _Journal of the American Medical Associa-_ _tion_ 316 (22): 2353- 54. Lusted, L. B. 1960. \"Logical Analysis in Roentgen Diagnosis.\" _Radiology_ 74:178- 93. Markov, John. 2015. _Machines of Loving Grace_ . New York: HarperCollins Publishers. Ng, Andrew. 2016. \"What Artificial Intelligence Can and Can't Do Right Now.\" _Harvard Business Review Online_ . Accessed Dec. 8, 2016. https:// hbr .org/ 2016/ 11 / what-artificial-intelligence-can-and-cant-do-right-now. Simon, H. A. 1951. \"A Formal Theory of the Employment Relationship.\" _Econo-_ _metrica_ 19 (3): 293- 305. Tadelis, S. 2002. \"Complexity, Flexibility and the Make-or-Buy Decision.\" _American_ _Economic Review_ 92 (2): 433- 37. Tirole, J. 2009. \"Cognition and Incomplete Contracts.\" _American Economic Review_ 99 (1): 265- 94. Varian, Hal R. 2014. \"Big Data: New Tricks for Econometrics.\" _Journal of Economic_ _Perspectives_ 28 (2): 3- 28.\n\n**Comment** Andrea Prat\n\nOne of the key activities of organizations is to collect, process, combine, and utilize information (Arrow 1974). A modern corporation exploits the vast amounts of data that it accumulates from marketing, operations, human resources, finance, and other functions to grow faster and be more\n\nAndrea Prat is the Richard Paul Richman Professor of Business at Columbia Business School and professor of economics at Columbia University. For acknowledgments, sources of research support, and disclosure of the author's material financial relationships, if any, please see http:// www .nber .org/ chapters/ c14022.ack.\n\nU.S. copyright law is illegal and injures the author and publisher.",
  "cleaned_len": 60563,
  "raw_len": 69055
}