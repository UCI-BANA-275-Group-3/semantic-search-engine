# Semantic Search Engine

## üöÄ Status: PRODUCTION READY

**Search Quality:** ‚úÖ **60.00%** (target: 55%)  
**Last Updated:** February 2, 2026

---

## Quick Start (3 Steps)

### 1. Start the Web UI
```powershell
cd C:\Git\semantic-search-engine
venv\Scripts\python -m streamlit run app/streamlit_app.py --server.port 8501
```

### 2. Open in Browser
Navigate to: **http://localhost:8501**

### 3. Search!
Type a query like "machine learning applications" and get instant results.

---

## Overview
Production-ready semantic search over a document corpus using embeddings + cosine similarity retrieval, plus optional LLM enhancement.

**Domain:** Academic papers (PDFs) from Zotero  
**Tech Stack:** SentenceTransformers, NumPy, Streamlit, Python 3.10+  
**Corpus:** 143 academic documents ‚Üí 9,807 text chunks  
**Search Speed:** ~500ms per query (top-10 results)

---

## Documentation

- **[DEPLOYMENT_SUMMARY.md](DEPLOYMENT_SUMMARY.md)** ‚Üê Start here for deployment info
- **[DEPLOYMENT_CHECKLIST.md](DEPLOYMENT_CHECKLIST.md)** ‚Üê Step-by-step deployment guide
- **[QUICKSTART.md](QUICKSTART.md)** ‚Üê Detailed pipeline walkthrough
- **[ARCHITECTURE.md](ARCHITECTURE.md)** ‚Üê System design & data flow

---
## TL;DR (For Graders)

This project implements a production-ready semantic search engine over 100+ academic papers.

**Core features**
- Embedding-based semantic retrieval
- Cosine similarity top-K search
- Optional LLM enhancement (summarize / QA / compare)
- Command-line interface (CLI)
**One-command demo**
```bash
python -m src.90_main --query "dynamic capabilities" --k 5 --enhance summarize
```
## Team Responsibilities
- **Alex:** Domain selection, data collection, preprocessing pipeline, and overall system design
- **Syeda:** Embedding implementation (model choice + vectorization) and cosine similarity search
- **Shweta:** LLM enhancement (summarize / QA / compare)
- **Neha:** CLI interface and optional UI deployment
- **Fatima:** Code quality, documentation, and deliverables coordination

---

## Status Note (Existing Corpus)
All Zotero exports should live under `corpus/raw/zotero/` as documented below.

---

## Repository Structure (Source of Truth)

```
semantic-search/
  README.md
  ARCHITECTURE.md
  TEAM_CONTRIBUTIONS.md
  requirements.txt

  corpus/
    raw/
      zotero/
        metadata/
          library.json            # Better BibLaTeX JSON export
        storage/
          <ATTACHMENT_KEY>/
            *.pdf
    derived/
      manifest/
        manifest.jsonl
      text/
        extracted.jsonl
        cleaned.jsonl
      chunks/
        chunks.jsonl
      embeddings/
        embeddings.npy
        index.jsonl

  src/
    00_build_manifest.py
    10_validate_corpus.py
    20_extract_text.py
    30_clean_text.py
    40_chunk_text.py
    50_embedding_backends.py
    60_embed_corpus.py
    70_similarity_search.py
    80_llm_enhancement.py
    90_main.py
```

**Notes**
- Files under `corpus/derived/` are generated artifacts.
- `ARCHITECTURE.md` documents design choices and pipeline details.
- `TEAM_CONTRIBUTIONS.md` lists who did what for peer evaluation.

---

## Pipeline Stages (Mapped to Source)

### **Manifest building**
- Script: `src/00_build_manifest.py`
- Inputs: `corpus/raw/zotero/metadata/library.json`, `corpus/raw/zotero/storage/**`
- Output: `corpus/derived/manifest/manifest.jsonl`

Completed Tasks: Build the manifest (00_build_manifest.py)

Define manifest schema (one record per document/attachment) and required fields (doc_id, title, creators, year, collection, attachment_path, mime, source_json, zotero_key, etc.).
Parse library.json (Better BibLaTeX JSON) to extract items + collections.
Resolve each item‚Äôs attachment(s) to corpus/raw/zotero/storage/<ATTACHMENT_KEY>/....
Normalize paths and generate a stable doc_id (e.g., hash of attachment path or item key + attachment key).
Write manifest.jsonl with one line per resolved attachment; log unresolved items to corpus/logs/.

### **Corpus validation**
- Script: `src/10_validate_corpus.py`
- Inputs: manifest + raw storage
- Outputs: `corpus/logs/validation_summary.json`, `corpus/logs/validation_errors.jsonl`, `corpus/logs/validation_warnings.jsonl`


Completed Tasks: Validate the corpus (10_validate_corpus.py)

Read the manifest and perform checks:
Paths exist and are readable
MIME types are supported (PDF/HTML)
No duplicate doc_ids
Minimum doc count ‚â• 100
Produce a summary report (counts, missing files, duplicates) and failure logs in corpus/logs/.
Exit non‚Äëzero if critical checks fail (missing attachments, too few docs), to gate the rest of the pipeline.

### **Text extraction**
- Script: `src/20_extract_text.py`
- Input: `corpus/derived/manifest/manifest.jsonl`
- Output: `corpus/derived/text/extracted.jsonl`

Completed Tasks: Text extraction (20_extract_text.py)
1) Read manifest.jsonl and iterate attachments with supported extensions.
2) Extract PDFs as Markdown with preserved headings (PyMuPDF4LLM).
3) Extract HTML/TXT as raw text.
4) Write extracted.jsonl with one record per attachment (or per page/section if needed).
5) Log extraction errors and warnings to corpus/logs/ (e.g., unsupported types, missing files).

### **Cleaning**
- Script: `src/30_clean_text.py`
- Input: `corpus/derived/text/extracted.jsonl`
- Output: `corpus/derived/text/cleaned.jsonl`

Completed Tasks: Cleaning (30_clean_text.py)
1) Read extracted.jsonl and preserve all provenance fields (doc_id, path, collections).
2) Clean page-by-page when available: remove boilerplate and metadata lines before stitching.
3) Stitch pages into one flow, repairing hyphenation across page breaks and normalizing Unicode (including Greek letters).
4) Keep paragraph breaks (`\\n\\n`) as structural anchors for chunking later.
5) Strip references/bibliography into `references_text` to keep the main body focused.
6) Normalize citations to `<CITATION>` to reduce noise in embeddings.
7) Record QA metrics (raw_len, cleaned_len) and a cleaning summary in `corpus/logs/clean_text_summary.json`.
8) Write cleaned.jsonl with one record per input record.
9) Log any cleaning exceptions to `corpus/logs/clean_text_errors.jsonl`.

### **Chunking**
- Script: `src/40_chunk_text.py`
- Input: `corpus/derived/text/cleaned.jsonl`
- Output: `corpus/derived/chunks/chunks.jsonl`

Plan: Chunking (40_chunk_text.py)
1) Read `corpus/derived/text/cleaned.jsonl` and keep paragraph breaks (`\\n\\n`) as *soft boundaries* when you split.
   - If a paragraph break is nearby, prefer splitting there instead of cutting a sentence.
2) Split into chunks by length (characters or tokens) with overlap, so context carries across boundaries.
3) Use a deterministic `chunk_id` format: `doc_id:chunk_index`.
4) Preserve metadata on each chunk (title, creators, year, collection, source path).
5) Store chunk text and basic stats (length, position) for debugging.
6) Write `corpus/derived/chunks/chunks.jsonl`.
7) Log any failures to `corpus/logs/chunk_text_errors.jsonl`.
Note: Use `cleaned_text` as the primary input for chunking. A single example of a cleaned record is saved at `corpus/derived/text/cleaned_example.json`.

### **Embeddings**
- Scripts: `src/50_embedding_backends.py`, `src/60_embed_corpus.py`
- Input: `corpus/derived/chunks/chunks.jsonl`
- Outputs: `corpus/derived/embeddings/embeddings.npy`, `corpus/derived/embeddings/index.jsonl`

Plan: Embeddings (50_embedding_backends.py, 60_embed_corpus.py)
1) Choose and configure a backend (Sentence Transformers / OpenAI / Word2Vec).
2) Embed each chunk text (batched) and persist vectors to embeddings.npy.
3) Write index.jsonl mapping row ‚Üí chunk_id + metadata pointer.
4) Validate embedding shapes and count alignment with chunks.jsonl.
5) Log backend config + model name to corpus/logs/ for reproducibility.

### **Similarity search**
- Script: `src/70_similarity_search.py`
- Inputs: embeddings artifacts
- Output: top-K chunks with cosine scores

Plan: Similarity search (70_similarity_search.py)
1) Load embeddings.npy and index.jsonl into memory.
2) Embed the query with the same backend and normalize if required.
3) Compute cosine similarity and return top-K results with scores.
4) Include chunk text + provenance for explainability.
5) (Optional) Add ANN index (FAISS/Annoy) if brute force is slow.

### **LLM enhancement (choose ONE)**
- Script: `src/80_llm_enhancement.py`
- Modes: summarize / QA / compare

Plan: LLM enhancement (80_llm_enhancement.py)
1) Accept top-K chunks and the user query.
2) Format a concise context window (truncate/merge as needed).
3) Run one chosen mode: summarize / QA / compare.
4) Return the LLM response + citations (chunk ids / titles).
5) Log prompt + model config for transparency.

### **CLI interface**
- Script: `src/90_main.py`
- Runs retrieval + optional enhancement

Plan: CLI interface (90_main.py)
1) Parse arguments (query, top_k, backend, enhance mode).
2) Run retrieval and print ranked results with scores.
3) Optionally call LLM enhancement and print final response.
4) Provide examples + sane defaults for the demo.
5) Exit with non-zero status on fatal errors.

### **Optional UI (Bonus)**
If we build a public UI (Streamlit/Gradio/HF Space), keep it minimal and demo-focused:
1) Simple search box + top-K slider.
2) Display ranked results with source metadata.
3) Toggle LLM enhancement mode (summarize/QA/compare).
4) Provide link to repo and demo video.

---

## Setup

### Python
- Recommended: Python 3.12

### Install dependencies
```bash
python3.12 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```
Note: A fresh install must include the required extraction dependencies (`pymupdf4llm`, `pymupdf_layout`, `tqdm`).

### Extraction dependencies (add as we grow)
- **PDFs:** `pymupdf4llm` (required by `src/20_extract_text.py`)
- **HTML/TXT:** stdlib only for now
- **Progress:** `tqdm` (required for status bar in extraction)

### Potential enhancements
- **Layout-aware PDFs:** `pymupdf_layout` for improved page structure in Markdown extraction (required).

### Environment variables (if using OpenAI)
Create `.env` (do not commit):
- `OPENAI_API_KEY=...`

---

## Running (Planned Commands)
```bash
python -m src.00_build_manifest \
  --zotero-metadata corpus/raw/zotero/metadata/library.json \
  --zotero-storage corpus/raw/zotero/storage \
  --out corpus/derived/manifest/manifest.jsonl

python -m src.20_extract_text \
  --manifest corpus/derived/manifest/manifest.jsonl \
  --out corpus/derived/text/extracted.jsonl

python -m src.30_clean_text \
  --in corpus/derived/text/extracted.jsonl \
  --out corpus/derived/text/cleaned.jsonl

python -m src.40_chunk_text \
  --in corpus/derived/text/cleaned.jsonl \
  --out corpus/derived/chunks/chunks.jsonl

python -m src.60_embed_corpus \
  --in corpus/derived/chunks/chunks.jsonl \
  --out corpus/derived/embeddings

python -m src.90_main \
  --query "dynamic capabilities" \
  --k 5 \
  --enhance summarize
```

---

## Integrating Into the Final App (Precompute vs. Search-Time)
You do **not** need to run the full pipeline every time you search. Use a two-step workflow:

**1) Offline / Precompute (run once or when corpus changes)**
- Run stages **00 ‚Üí 60** to generate artifacts:
  - `corpus/derived/manifest/manifest.jsonl`
  - `corpus/derived/text/extracted.jsonl`
  - `corpus/derived/text/cleaned.jsonl`
  - `corpus/derived/chunks/chunks.jsonl`
  - `corpus/derived/embeddings/embeddings.npy`
  - `corpus/derived/embeddings/index.jsonl`

**2) Search-Time (run per query)**
- Run **70 ‚Üí 90** to load artifacts and answer queries:
  - `src/70_similarity_search.py`
  - `src/80_llm_enhancement.py` (optional)
  - `src/90_main.py`

This keeps the app fast and avoids recomputing embeddings for every query.

---

## Deliverables
- `README.md` (project overview, setup, examples)
- `ARCHITECTURE.md` (technical details)
- `TEAM_CONTRIBUTIONS.md` (who did what + peer-eval support)
- `requirements.txt`
- Source code under `src/`
- 2‚Äì3 minute demo video
- Peer assessment form (each member)
